{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Workflow & Intro to Data Preperation & Feature Engineering\n",
    "\n",
    "Slides Modified from Monique.\n",
    "\n",
    "#### Agenda\n",
    "\n",
    "1. Machine Learning Workflow\n",
    "2. Data Preperation\n",
    "    - Dealing with Outliers\n",
    "    - Dealing with Null Values\n",
    "    - Variable Transformations\n",
    "3. Feature Engineering Overview and Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Machine Learning Workflow\n",
    "\n",
    "<img src='imgs/data-science-explore.png' width=800>\n",
    "\n",
    "- Iterative process\n",
    "- Non-linear process\n",
    "- Lots of judgement and refining along the way\n",
    "- Lots of time spent in data prep\n",
    "- \"Big data\": a lot of time can be spent in data retrieval\n",
    "\n",
    "Source: Practical Machine Learning with Python, Apress/Springer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "**Machine Learning Workflow**\n",
    "\n",
    "<img src='imgs/data-science-explore.png' width=600>\n",
    "\n",
    "### Data Retrieval\n",
    "- SQL, APIs, Web Scraping, csv, Excel...\n",
    "- Could include combining some of the above\n",
    "- Also called \"Data Ingestion\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "**Machine Learning Workflow**\n",
    "\n",
    "<img src='imgs/data-science-explore.png' width=600>\n",
    "\n",
    "### Data Preparation\n",
    "- **Processing and Wrangling**: You became `pandas` experts last week.\n",
    "- **Feature extraction and engineering**: Will go over this today. What features (i.e., variables, `x`) do I need for my problem?\n",
    "- **Feature selection**: To be covered later today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "**Machine Learning Workflow**\n",
    "\n",
    "<img src='imgs/data-science-explore.png' width=600>\n",
    "\n",
    "### Modeling (i.e., machine learning)\n",
    "- `scikit-learn` being the main basic package\n",
    "- Other packages for deep learning\n",
    "- Supervised vs. unsupervised learning\n",
    "- \"Build a model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "**Machine Learning Workflow**\n",
    "\n",
    "<img src='imgs/data-science-explore.png' width=600>\n",
    "\n",
    "### Machine Learning Algorithm\n",
    "- **\"Algorithm\"**: series of steps based on rules that a computer takes to calculate something\n",
    "- Within supervised:\n",
    "    - Regression: `y` is a continuous number (e.g., price)\n",
    "    - Classification: `y` is discrete (e.g., customer retained or not)\n",
    "- Examples: decision trees, linear regression, neural networks\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "**Machine Learning Workflow**\n",
    "\n",
    "<img src='imgs/data-science-explore.png' width=600>\n",
    "\n",
    "### Model Evaluation & Tuning\n",
    "- Our first model will probably not be the best model; need to pick\n",
    "- **Evaluation**: Using metrics to pick the best model for the use case\n",
    "- **Tuning**: Besides picking between algorithms, there are 'knobs' / settings to 'tune' a model for a specific algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "**Machine Learning Workflow**\n",
    "<img src='imgs/data-science-explore.png' width=600>\n",
    "\n",
    "### Deployment & Monitoring\n",
    "- We picked a model and it's ready for use by our users\n",
    "- Be careful about concept drift\n",
    "- Models sometimes need to be re-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "## Types of Questions\n",
    "\n",
    "\n",
    "| Type of question | Description | Example |\n",
    "|:---|:--------------------------|:----------------|\n",
    "| **Descriptive** | Summarize a characteristic of a set of data| Proportion of males, the mean number of servings of fresh fruits and vegetables per day |\n",
    "| **Exploratory** | Analyze the data to see if there are patterns, trends, or relationships between variables; “hypothesis-generating” analyses|If you had a general thought that diet was linked somehow to viral illnesses, start by examining relationships between a range of dietary factors and viral illnesses|\n",
    "| **Inferential** | Testing a hypothesis, statistically |Analyzing data for a subset / sample of the population and generalizing insights for the general population; Is there a higher incidence of cancer for women than for men?|\n",
    "| **Predictive**  | Predicting a value, not necessarily figuring out why| Predicting cancer diagnosis from x-rays using computer vision|\n",
    "| **Causal**      | Whether changing one factor will change another factor | Does changing diet lead to higher incidence of cancer?|\n",
    "| **Mechanistic** | Understanding *how* one factor changes another | How does diet lead to higher incidence of cancer? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "## Data Preperation\n",
    "\n",
    "The main goal of this phase is to prepare the data for exploratory data analysis, inferential analysis, or predeiciton (modelling). In other words, we're making sure our data is in good shape, we have treated our missing values, dealt with weird data, cleaned it up.\n",
    "\n",
    "Common Data Preperation Techniques:\n",
    "- Outlier Detection\n",
    "- Handling Null Values\n",
    "- Variable Transformation\n",
    "\n",
    "\n",
    "### Outlier Detection and Handling Outliers\n",
    "\n",
    "- Data is not always right\n",
    "- Could be human error, could be system error\n",
    "- **Outlier**: an observation point that is distant from other observations\n",
    "- Helpful to pointing us what can be wrong\n",
    "- **Some errors are obvious; many require interviewing the domain experts to figure out**\n",
    "\n",
    "Note: Before deleting outliers ask yourself if this is needed, is the outlier nessecarry. It depends on your use case, your business problem, where outliers may be important. An example is fraud detection. \n",
    "\n",
    "### Outlier Detection: demo\n",
    "- Docs: https://scikit-learn.org/stable/datasets/index.html#boston-dataset\n",
    "- [Example source](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- CRIM per capita crime rate by town\n",
    "\n",
    "- ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "- INDUS proportion of non-retail business acres per town\n",
    "\n",
    "- CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "\n",
    "- NOX nitric oxides concentration (parts per 10 million)\n",
    "\n",
    "- RM average number of rooms per dwelling\n",
    "\n",
    "- AGE proportion of owner-occupied units built prior to 1940\n",
    "\n",
    "- DIS weighted distances to five Boston employment centres\n",
    "\n",
    "- RAD index of accessibility to radial highways\n",
    "\n",
    "- TAX full-value property-tax rate per $10,000\n",
    "\n",
    "- PTRATIO pupil-teacher ratio by town\n",
    "\n",
    "- B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "\n",
    "- LSTAT % lower status of the population\n",
    "\n",
    "- MEDV Median value of owner-occupied homes in $1000’s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation: https://scikit-learn.org/stable/datasets/toy_dataset.html#boston-dataset\n",
    "boston = load_boston()\n",
    "x = boston.data\n",
    "y = boston.target\n",
    "columns = boston.feature_names\n",
    "\n",
    "#create the dataframe\n",
    "boston_df = pd.DataFrame(boston.data)\n",
    "boston_df.columns = columns\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1: Summary of the data**\n",
    "\n",
    "- Use your intuition\n",
    "- Ask a domain expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2: Visualizing a Single Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=boston_df['DIS']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 3: Visualizing Multi-Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.scatter(boston_df['CRIM'], y)\n",
    "ax.set_xlabel('Per capita crime rate by town')\n",
    "ax.set_ylabel('Median value of owner-occupied homes in $1000’s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 4: Z-Score**\n",
    "\n",
    "A way to detect outliers is to remove values with a z-score greater than 3. The z-score is measured in terms of standard deviations from the mean.\n",
    "\n",
    "- Z-score of 0 indicates the value is the mean\n",
    "- Z-score of 1 indicates the value is within 1 standard deviation from the mean. \n",
    "- Z-score of 2 indicates the value is within 2 standard deviations from the mean.\n",
    "- Z-score of 3 indicates the value is within 3 standard deviations from the mean.\n",
    "- **Z-score of above 3 indicates the value is greater than 3 standard deviations from the mean. Data Scinetist often label values with a z-score above 3 as outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#Finding Z Score on Column\n",
    "stats.zscore(boston_df['ZN'])\n",
    "\n",
    "#Turning Absolute\n",
    "np.abs(stats.zscore(boston_df['ZN']))\n",
    "\n",
    "#(np.abs(stats.zscore(boston_df['ZN'])) > 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution?\n",
    "\n",
    "- Can drop the observation\n",
    "- Can replace the outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Handling Null Values\n",
    "\n",
    "Many times we will be handed data with missing data or corrupted data. Most commonly, missing data are represented as NaNs. NaNs are blank elements in Pandas. \n",
    "\n",
    "- It can be a system error that causes missing values, or it wasn't captured.\n",
    "- There are techinques to deal with missing data, but all of them are imperfect. \n",
    "\n",
    "Resource: https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/\n",
    "\n",
    "### Null values: Demo\n",
    "- Dataset: https://www.kaggle.com/uciml/pima-indians-diabetes-database/data#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "diabetes_df = pd.read_csv('diabetes.csv')\n",
    "diabetes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check percentage of data missing for each feature/column\n",
    "round(100*(diabetes_df.isnull().sum()/len(diabetes_df)),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values: Summary of the data\n",
    "- Sometimes null values aren't exactly NaNs\n",
    "- They are encoded as -1 or 9999 etc.\n",
    "- Sometimes it's 0. \n",
    "- Does 0 make sense for some of these categories??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values: Encoding true NaNs as NaNs\n",
    "- Won't be used in summary calculations (e.g., average, count)\n",
    "- Some columns have a lot of what we think could be missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_missing_vals = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'] # cols with missing values\n",
    "(diabetes_df[cols_missing_vals] == 0).sum() # count number of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df[cols_missing_vals] = diabetes_df[cols_missing_vals].replace(0, np.NaN) # replace 0's with NaNs\n",
    "diabetes_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values: Dropping Mssing Values\n",
    "- Could be a good idea if there aren't too many records removed\n",
    "- Let's do this for Glucose and BMI columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape before dropping NAs\", diabetes_df.shape)\n",
    "\n",
    "diabetes_df = diabetes_df.dropna(subset=['Glucose', 'BMI']) # drop rows with Glucose and BMI as NaN\n",
    "\n",
    "print(\"Shape after dropping NAs for Glucose and BMI columns\", diabetes_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null values: using the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values with the average\n",
    "diabetes_df['SkinThickness'] = diabetes_df['SkinThickness'].fillna(value=diabetes_df['SkinThickness'].mean())\n",
    "diabetes_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Variable Transformation\n",
    "\n",
    "- Basic transformations (e.g., logarithmic (making it more normally distributed))\n",
    "- Binning (e.g., grouping numbers into bins)\n",
    "- Scaling (e.g., setting everything between 0 and 1)\n",
    "- Dummy variables (e.g., turning categories into multiple columns of binary variables) - BE CAREFUL\n",
    "\n",
    "Will learn more when we get into `scikit-learn` library and dive into unsupervised and supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning with q-cut\n",
    "pd.qcut(diabetes_df['Age'], q = 4).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning with cut\n",
    "pd.cut(diabetes_df['Age'], bins = 4).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool Data Analysis Tool - Pandas Profiling\n",
    "\n",
    "Very useful! Great for exploratory data analysis.\n",
    "\n",
    "`conda install -c conda-forge pandas-profiling`\n",
    "\n",
    "Alternatives to Pandas Profiling: Sweetviz\n",
    "\n",
    "Check out more here: https://towardsdatascience.com/data-frame-eda-packages-comparison-pandas-profiling-sweetviz-and-pandasgui-bbab4841943b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "prof = ProfileReport(diabetes_df)\n",
    "prof.to_file(output_file='output.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "- A key part to any DS Job is to figure out which parts are relevant to our desired outcome.\n",
    "- The goal is to make the simplest model possible with the hihgest predictive power.\n",
    "- Example: If we determine the cause of sales at a cafe is determined by two variables, price and the weather, we have a lot more predictive power and leverage than a model with thousands of variables.\n",
    "- However, sometimes the a thousand variable model is needed to explain the data.\n",
    "\n",
    "- Feature engineering is like making an argument for an essay. There is a lot of things with varying relevance that can be included, the hard part is choosing the most relevant/correct ones, synthesizing different arguments into one. \n",
    "\n",
    "- The best features are domain and problem specific. \n",
    "- Good features ideally:\n",
    "    - Capture most important aspects of a problem\n",
    "    - allow learning with a few examples\n",
    "    - generalize to new scenarios. \n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1. Taking a date and extracting out the week number, weekday, month etc.\n",
    "    - Sales are often based on seasonality. \n",
    "2. Taking freeform text (tweets) and extracting the number of words, hashtags, emojis, and counts of words etc.\n",
    "    -  Text \"metadata' can sometimes help with sentiment anlaysis\n",
    "    \n",
    "3. Take geographical coordintes and getting continent, country, urban vs. rural.\n",
    "    - Housing price can depend on features extracted from geographical coordinates.\n",
    "4. Predicting NBA games, we might extract the stats of the players, and coaches, and maybe look at the recent games. Home or Away games. \n",
    "\n",
    "\n",
    "\n",
    "**Feature Engineering vs. Feature Selection**\n",
    "\n",
    "Through feature engineering we usually add more features to our data to make it more complex. In Feature selection, we are trying to choose thevbest features and remove features that do not add anything to our model. One common method is to remove features that have a low variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Exercise (15 minutes)\n",
    "\n",
    "You're presented with the data below.\n",
    "\n",
    "Think of at least 5 features you might add.\n",
    "\n",
    "**Note: For this exercise you will be creating new columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "retail_df = pd.DataFrame([['Protein Bar','25-01-2021', 2.99, 1024, 1],\\\n",
    "              ['Oat Milk','25-01-2021', 3.99, 729, 1],\\\n",
    "              ['Banana','25-01-2021', 1.99, 256, 1]],\\\n",
    "            columns=['Item', 'Date', 'Price', 'Sales', 'Store Id'])\n",
    "\n",
    "retail_df.loc[:,'Date'] = pd.to_datetime(retail_df['Date'])\n",
    "retail_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translator\n",
    "\n",
    "Dataset was taken from Kaggle and can be found [**here**](https://www.kaggle.com/jannesklaas/frenchenglish-bilingual-pairs?select=fra.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "import io\n",
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"fra.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>FR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          FR\n",
       "0   Hi.      Salut!\n",
       "1  Run!     Cours !\n",
       "2  Run!    Courez !\n",
       "3  Who?       Qui ?\n",
       "4  Wow!  Ça alors !"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={'English words/sentences': 'EN',\n",
    "                   'French words/sentences': 'FR'},\n",
    "          inplace=True, errors='raise')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls in EN: 0\n",
      "Number of nulls in FR: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of nulls in EN: {}'.format(data['EN'].isnull().sum()))\n",
    "print('Number of nulls in FR: {}'.format(data['FR'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175621, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.iloc[:20000,:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>FR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>It's a pipe dream.</td>\n",
       "      <td>C'est un projet chimérique.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>It's a rented car.</td>\n",
       "      <td>C'est une voiture de location.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>It's a small town.</td>\n",
       "      <td>C'est une petite ville.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>It's a true story.</td>\n",
       "      <td>C'est une histoire vraie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>It's all I can do.</td>\n",
       "      <td>C'est tout ce que je peux faire.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       EN                                FR\n",
       "19995  It's a pipe dream.       C'est un projet chimérique.\n",
       "19996  It's a rented car.    C'est une voiture de location.\n",
       "19997  It's a small town.           C'est une petite ville.\n",
       "19998  It's a true story.         C'est une histoire vraie.\n",
       "19999  It's all I can do.  C'est tout ce que je peux faire."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['En_clean'] = df['EN'].apply(lambda x: remove_punct(x))\n",
    "df['Fr_clean'] = df['FR'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "df['En_tokenized'] = df['En_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "df['Fr_tokenized'] = df['Fr_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as sp\n",
    " \n",
    "def write_trainer_file(col, filename):\n",
    "    texts = list(col.values)\n",
    "    with open(filename, 'w',encoding='utf-8') as f:\n",
    "        for text in texts:\n",
    "            f.write(text + \"\\n\")\n",
    "            \n",
    "#use sentences from train_df as our training data\n",
    "en_sp_trainer = \"en_spm.txt\"\n",
    "fr_sp_trainer = \"fr_spm.txt\"\n",
    "write_trainer_file(df[\"EN\"], en_sp_trainer)\n",
    "write_trainer_file(df[\"FR\"], fr_sp_trainer)\n",
    " \n",
    "#create our English SentencePiece model\n",
    "sp_en_train_param = f\"--input={en_sp_trainer} --model_prefix=en_sp --vocab_size=3207\"\n",
    "sp.SentencePieceTrainer.Train(sp_en_train_param)\n",
    "en_sp = sp.SentencePieceProcessor()\n",
    "en_sp.Load(\"en_sp.model\")\n",
    "\n",
    "#create our French SentencePiece model\n",
    "sp_fr_train_param = f\"--input={fr_sp_trainer} --model_prefix=nl_sp --vocab_size=5454\"\n",
    "sp.SentencePieceTrainer.Train(sp_fr_train_param)\n",
    "fr_sp = sp.SentencePieceProcessor()\n",
    "fr_sp.Load(\"nl_sp.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Thi', 's', '▁is', '▁a', '▁test', '.']\n",
      "[72, 6, 18, 8, 1510, 3]\n",
      "This is a test.\n"
     ]
    }
   ],
   "source": [
    "print(en_sp.EncodeAsPieces(\"This is a test.\"))\n",
    "print(en_sp.EncodeAsIds(\"This is a test.\"))\n",
    "print(en_sp.DecodeIds(en_sp.EncodeAsIds(\"This is a test.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-45442211d0b8>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{lang}_pieces\"] = lang_pieces\n",
      "<ipython-input-14-45442211d0b8>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{lang}_len\"] = lang_lens\n"
     ]
    }
   ],
   "source": [
    "def encode_sentence(df, lang, spm):\n",
    "    lang_pieces = []\n",
    "    lang_lens = []\n",
    "    for index, row in df.iterrows():\n",
    "        lang_piece = spm.EncodeAsIds(row[lang])\n",
    "        lang_pieces.append(lang_piece)\n",
    "        lang_lens.append(len(lang_piece)) \n",
    "    df[f\"{lang}_pieces\"] = lang_pieces\n",
    "    df[f\"{lang}_len\"] = lang_lens\n",
    " \n",
    "encode_sentence(df, \"EN\", en_sp)\n",
    "encode_sentence(df, \"FR\", fr_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "def plotLangLen(lang1, lang2):\n",
    "    trace1 = go.Histogram(\n",
    "        x=df[f\"{lang1}_len\"].values,\n",
    "        opacity=0.75,\n",
    "        name = f\"Length of {lang1} sentences\",\n",
    "        marker=dict(color='rgba(171, 50, 96, 0.6)'))\n",
    "    trace2 = go.Histogram(\n",
    "        x=df[f\"{lang2}_len\"].values,\n",
    "        opacity=0.75,\n",
    "        name = f\"Length of {lang2} sentences\",\n",
    "        marker=dict(color='rgba(12, 50, 196, 0.6)'))\n",
    " \n",
    "    data = [trace1, trace2]\n",
    "    layout = go.Layout(barmode='overlay',\n",
    "                       title=f\"Lengths of {lang1} and {lang2} sentences\",\n",
    "                       xaxis=dict(title='Length'),\n",
    "                       yaxis=dict( title='Count'),\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig, config={'showLink': True})\n",
    " \n",
    "plotLangLen(\"EN\", \"FR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_size = en_sp.get_piece_size()\n",
    "fr_vocab_size = fr_sp.get_piece_size()\n",
    "print(f\"EN vocab size: {en_vocab_size}\")\n",
    "print(f\"FR vocab size: {fr_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "en_vocab_size = en_sp.get_piece_size()\n",
    "fr_vocab_size = fr_sp.get_piece_size()\n",
    "\n",
    "en_max_length = df[\"EN_len\"].max()\n",
    "fr_max_length = df[\"FR_len\"].max()\n",
    "\n",
    "#we use 30 as length here, to shorten processing time\n",
    "en_max_length=30\n",
    "fr_max_length=en_max_length\n",
    "\n",
    "#use post padding to fill up short sentence with 0\n",
    "en_padded_seq = pad_sequences(df[\"EN_pieces\"].tolist(), maxlen=en_max_length, padding='post')\n",
    "fr_padded_seq = pad_sequences(df[\"FR_pieces\"].tolist(), maxlen=fr_max_length, padding='post')\n",
    "train_seq_df = pd.DataFrame( {'en_seq':en_padded_seq.tolist(), 'fr_seq':fr_padded_seq.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_seq_df.head())\n",
    "display(train_seq_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(input_vocab,output_vocab, input_length,output_length,output_dim):\n",
    "      model = Sequential()\n",
    "      #mark_zero , set 0 as special character reserved for unknown words  \n",
    "      model.add(Embedding(input_vocab, output_dim, input_length=input_length, mask_zero=True))\n",
    "      model.add(LSTM(output_dim))\n",
    "      #repeat the input (n) times\n",
    "      model.add(RepeatVector(output_length))\n",
    "    #return the full sequences\n",
    "      model.add(LSTM(output_dim, return_sequences=True))\n",
    "      #model.add(TimeDistributed(Dense(output_vocab, activation='softmax')))\n",
    "      \n",
    "      model.add(Dense(output_vocab, activation='softmax'))\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(train_seq_df, test_size=0.1, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.asarray(train[\"fr_seq\"].tolist())\n",
    "trainY = np.asarray(train[\"en_seq\"].tolist())\n",
    "\n",
    "testX = np.asarray(test[\"fr_seq\"].tolist())\n",
    "testY = np.asarray(test[\"en_seq\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(fr_vocab_size, en_vocab_size, fr_max_length, en_max_length, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_size):\n",
    "   ylist = list()\n",
    "   for sequence in sequences:\n",
    "    encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "    ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'nmt_model'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# train model\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "                    epochs=15, batch_size=64, validation_split = 0.1,callbacks=[checkpoint], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(train_seq_df, test_size=0.1, random_state = 3)\n",
    "trainX = np.asarray(train[\"fr_seq\"].tolist())\n",
    "trainY = np.asarray(train[\"en_seq\"].tolist())\n",
    "testX = np.asarray(test[\"fr_seq\"].tolist())\n",
    "testY = np.asarray(test[\"en_seq\"].tolist())\n",
    "#sparse_categorical_crossentropy for densed target output as integers\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n",
    "filename = 'nmt_model'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# train model\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "                    epochs=15, batch_size=64, validation_split = 0.1,callbacks=[checkpoint], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

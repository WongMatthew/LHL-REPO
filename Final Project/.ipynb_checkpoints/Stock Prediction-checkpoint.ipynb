{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a50b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "d91578af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200)\n",
    "tf.random.set_seed(200)\n",
    "random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "7f3dbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 50\n",
    "\n",
    "LOOKUP_STEP = 15\n",
    "\n",
    "SCALE = True\n",
    "scale_str = f\"scale-{int(SCALE)}\"\n",
    "\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"shuffle-{int(SHUFFLE)}\"\n",
    "\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"split_date-{int(SPLIT_BY_DATE)}\"\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "\n",
    "date_now = time.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "bffc6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "N_LAYERS = 2\n",
    "CELL = LSTM\n",
    "UNITS = 256\n",
    "DROPOUT = 0.7\n",
    "BIDIRECTIONAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "76d5ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 800\n",
    "epochs = f\"epochs-{int(EPOCHS)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "fe672cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Ticker\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-{epochs}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf018e6b",
   "metadata": {},
   "source": [
    "* ticker (str/pd.DataFrame)\n",
    "* n_steps (int) - default = 50\n",
    "* scale (bool) - default = True\n",
    "* shuffle (bool) - default = True\n",
    "* lookup_step (int) - default = 1 (e.g next day)\n",
    "* split_by_date (bool)\n",
    "* test_size (float) - default = 0.2 \n",
    "* feature_columns (list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "50fd4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle two arrays the same\n",
    "def same_shuffle(a, b):\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "4c9d581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nflx = si.get_quote_table('nflx' , dict_result = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "357585f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nflx2 = si.get_data('nflx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "b952010f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1y Target Est</td>\n",
       "      <td>611.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52 Week Range</td>\n",
       "      <td>458.60 - 598.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ask</td>\n",
       "      <td>0.00 x 1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avg. Volume</td>\n",
       "      <td>3331401.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beta (5Y Monthly)</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bid</td>\n",
       "      <td>0.00 x 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Day's Range</td>\n",
       "      <td>583.14 - 591.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EPS (TTM)</td>\n",
       "      <td>9.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Earnings Date</td>\n",
       "      <td>Oct 18, 2021 - Oct 22, 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ex-Dividend Date</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Forward Dividend &amp; Yield</td>\n",
       "      <td>N/A (N/A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Market Cap</td>\n",
       "      <td>261.366B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Open</td>\n",
       "      <td>585.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PE Ratio (TTM)</td>\n",
       "      <td>61.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Previous Close</td>\n",
       "      <td>588.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Quote Price</td>\n",
       "      <td>590.530029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Volume</td>\n",
       "      <td>2684005.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   attribute                        value\n",
       "0              1y Target Est                       611.49\n",
       "1              52 Week Range              458.60 - 598.76\n",
       "2                        Ask                  0.00 x 1300\n",
       "3                Avg. Volume                    3331401.0\n",
       "4          Beta (5Y Monthly)                         0.76\n",
       "5                        Bid                  0.00 x 1000\n",
       "6                Day's Range              583.14 - 591.79\n",
       "7                  EPS (TTM)                         9.65\n",
       "8              Earnings Date  Oct 18, 2021 - Oct 22, 2021\n",
       "9           Ex-Dividend Date                          NaN\n",
       "10  Forward Dividend & Yield                    N/A (N/A)\n",
       "11                Market Cap                     261.366B\n",
       "12                      Open                        585.8\n",
       "13            PE Ratio (TTM)                        61.19\n",
       "14            Previous Close                       588.55\n",
       "15               Quote Price                   590.530029\n",
       "16                    Volume                    2684005.0"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nflx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "a78846ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    \n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    result['last_sequence'] = last_sequence\n",
    "    \n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # split dataset by date \n",
    "    if split_by_date:\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "ffac8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "98223e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "852b5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, pred_future, true_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, pred_future, true_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "b3ec0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    #plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "48bb64f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "a413092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/450\n",
      "76/76 [==============================] - 38s 452ms/step - loss: 0.0018 - mean_absolute_error: 0.0272 - val_loss: 2.2945e-04 - val_mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00023, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 2/450\n",
      "76/76 [==============================] - 37s 490ms/step - loss: 9.3911e-04 - mean_absolute_error: 0.0224 - val_loss: 3.3889e-04 - val_mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00023\n",
      "Epoch 3/450\n",
      "76/76 [==============================] - 33s 439ms/step - loss: 7.4759e-04 - mean_absolute_error: 0.0191 - val_loss: 5.7785e-04 - val_mean_absolute_error: 0.0182\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00023\n",
      "Epoch 4/450\n",
      "76/76 [==============================] - 35s 456ms/step - loss: 6.2812e-04 - mean_absolute_error: 0.0177 - val_loss: 1.8795e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00023 to 0.00019, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 5/450\n",
      "76/76 [==============================] - 33s 437ms/step - loss: 6.1065e-04 - mean_absolute_error: 0.0168 - val_loss: 2.2468e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00019\n",
      "Epoch 6/450\n",
      "76/76 [==============================] - 29s 382ms/step - loss: 7.3795e-04 - mean_absolute_error: 0.0185 - val_loss: 2.1892e-04 - val_mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00019\n",
      "Epoch 7/450\n",
      "76/76 [==============================] - 33s 434ms/step - loss: 7.7617e-04 - mean_absolute_error: 0.0204 - val_loss: 3.0775e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00019\n",
      "Epoch 8/450\n",
      "76/76 [==============================] - 35s 464ms/step - loss: 5.5624e-04 - mean_absolute_error: 0.0168 - val_loss: 2.4876e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00019\n",
      "Epoch 9/450\n",
      "76/76 [==============================] - 31s 404ms/step - loss: 5.7946e-04 - mean_absolute_error: 0.0175 - val_loss: 2.3791e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00019\n",
      "Epoch 10/450\n",
      "76/76 [==============================] - 27s 361ms/step - loss: 5.3712e-04 - mean_absolute_error: 0.0163 - val_loss: 3.4955e-04 - val_mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00019\n",
      "Epoch 11/450\n",
      "76/76 [==============================] - 30s 394ms/step - loss: 5.6176e-04 - mean_absolute_error: 0.0172 - val_loss: 2.0634e-04 - val_mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00019\n",
      "Epoch 12/450\n",
      "76/76 [==============================] - 31s 404ms/step - loss: 5.6293e-04 - mean_absolute_error: 0.0172 - val_loss: 2.3927e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00019\n",
      "Epoch 13/450\n",
      "76/76 [==============================] - 28s 375ms/step - loss: 5.0214e-04 - mean_absolute_error: 0.0163 - val_loss: 3.1313e-04 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00019\n",
      "Epoch 14/450\n",
      "76/76 [==============================] - 30s 390ms/step - loss: 5.6526e-04 - mean_absolute_error: 0.0174 - val_loss: 2.3979e-04 - val_mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00019\n",
      "Epoch 15/450\n",
      "76/76 [==============================] - 30s 396ms/step - loss: 5.6973e-04 - mean_absolute_error: 0.0177 - val_loss: 2.0372e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00019\n",
      "Epoch 16/450\n",
      "76/76 [==============================] - 26s 342ms/step - loss: 5.4459e-04 - mean_absolute_error: 0.0170 - val_loss: 2.7032e-04 - val_mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00019\n",
      "Epoch 17/450\n",
      "76/76 [==============================] - 24s 317ms/step - loss: 5.2931e-04 - mean_absolute_error: 0.0171 - val_loss: 1.8024e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00019 to 0.00018, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 18/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 4.8320e-04 - mean_absolute_error: 0.0173 - val_loss: 2.2865e-04 - val_mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00018\n",
      "Epoch 19/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 5.0297e-04 - mean_absolute_error: 0.0172 - val_loss: 2.9482e-04 - val_mean_absolute_error: 0.0130\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00018\n",
      "Epoch 20/450\n",
      "76/76 [==============================] - 25s 328ms/step - loss: 4.7691e-04 - mean_absolute_error: 0.0167 - val_loss: 1.9242e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00018\n",
      "Epoch 21/450\n",
      "76/76 [==============================] - 24s 309ms/step - loss: 4.5126e-04 - mean_absolute_error: 0.0163 - val_loss: 1.7965e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00018 to 0.00018, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 22/450\n",
      "76/76 [==============================] - 24s 322ms/step - loss: 4.7136e-04 - mean_absolute_error: 0.0165 - val_loss: 3.0183e-04 - val_mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00018\n",
      "Epoch 23/450\n",
      "76/76 [==============================] - 24s 322ms/step - loss: 4.6765e-04 - mean_absolute_error: 0.0167 - val_loss: 1.9125e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00018\n",
      "Epoch 24/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 5.2533e-04 - mean_absolute_error: 0.0172 - val_loss: 4.7495e-04 - val_mean_absolute_error: 0.0162\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00018\n",
      "Epoch 25/450\n",
      "76/76 [==============================] - 25s 330ms/step - loss: 5.3267e-04 - mean_absolute_error: 0.0179 - val_loss: 3.4462e-04 - val_mean_absolute_error: 0.0126\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00018\n",
      "Epoch 26/450\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 4.7308e-04 - mean_absolute_error: 0.0172 - val_loss: 2.0853e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00018\n",
      "Epoch 27/450\n",
      "76/76 [==============================] - 25s 324ms/step - loss: 4.5096e-04 - mean_absolute_error: 0.0167 - val_loss: 2.2158e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00018\n",
      "Epoch 28/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 4.7384e-04 - mean_absolute_error: 0.0171 - val_loss: 1.9099e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00018\n",
      "Epoch 29/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 4.4658e-04 - mean_absolute_error: 0.0170 - val_loss: 1.7733e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00018 to 0.00018, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 30/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 4.1492e-04 - mean_absolute_error: 0.0162 - val_loss: 2.4124e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00018\n",
      "Epoch 31/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 4.2303e-04 - mean_absolute_error: 0.0164 - val_loss: 2.6964e-04 - val_mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00018\n",
      "Epoch 32/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 4.7253e-04 - mean_absolute_error: 0.0174 - val_loss: 2.4169e-04 - val_mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00018\n",
      "Epoch 33/450\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 4.9034e-04 - mean_absolute_error: 0.0180 - val_loss: 1.9321e-04 - val_mean_absolute_error: 0.0120\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00018\n",
      "Epoch 34/450\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 4.3838e-04 - mean_absolute_error: 0.0166 - val_loss: 1.7452e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00018 to 0.00017, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 35/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 24s 316ms/step - loss: 4.4949e-04 - mean_absolute_error: 0.0166 - val_loss: 1.8150e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00017\n",
      "Epoch 36/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 4.4306e-04 - mean_absolute_error: 0.0168 - val_loss: 2.0133e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00017\n",
      "Epoch 37/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 4.4169e-04 - mean_absolute_error: 0.0170 - val_loss: 1.6754e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00017 to 0.00017, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 38/450\n",
      "76/76 [==============================] - 25s 324ms/step - loss: 4.2084e-04 - mean_absolute_error: 0.0165 - val_loss: 2.1713e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00017\n",
      "Epoch 39/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 4.4132e-04 - mean_absolute_error: 0.0168 - val_loss: 2.1862e-04 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00017\n",
      "Epoch 40/450\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 4.0029e-04 - mean_absolute_error: 0.0162 - val_loss: 2.1621e-04 - val_mean_absolute_error: 0.0121\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00017\n",
      "Epoch 41/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 4.2724e-04 - mean_absolute_error: 0.0167 - val_loss: 2.4285e-04 - val_mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00017\n",
      "Epoch 42/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 3.6125e-04 - mean_absolute_error: 0.0159 - val_loss: 2.2458e-04 - val_mean_absolute_error: 0.0119\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00017\n",
      "Epoch 43/450\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 4.0612e-04 - mean_absolute_error: 0.0164 - val_loss: 2.7100e-04 - val_mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00017\n",
      "Epoch 44/450\n",
      "76/76 [==============================] - 22s 287ms/step - loss: 4.2240e-04 - mean_absolute_error: 0.0169 - val_loss: 3.4081e-04 - val_mean_absolute_error: 0.0138\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00017\n",
      "Epoch 45/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 4.2441e-04 - mean_absolute_error: 0.0169 - val_loss: 1.8419e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00017\n",
      "Epoch 46/450\n",
      "76/76 [==============================] - 25s 336ms/step - loss: 4.1434e-04 - mean_absolute_error: 0.0167 - val_loss: 2.3814e-04 - val_mean_absolute_error: 0.0131\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00017\n",
      "Epoch 47/450\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 3.7503e-04 - mean_absolute_error: 0.0159 - val_loss: 1.5210e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00017 to 0.00015, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 48/450\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 4.0782e-04 - mean_absolute_error: 0.0162 - val_loss: 1.6135e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00015\n",
      "Epoch 49/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 4.0526e-04 - mean_absolute_error: 0.0163 - val_loss: 2.1475e-04 - val_mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00015\n",
      "Epoch 50/450\n",
      "76/76 [==============================] - 24s 311ms/step - loss: 3.6466e-04 - mean_absolute_error: 0.0157 - val_loss: 1.5100e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.00015 to 0.00015, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 51/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 3.9508e-04 - mean_absolute_error: 0.0164 - val_loss: 1.7014e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00015\n",
      "Epoch 52/450\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 3.8999e-04 - mean_absolute_error: 0.0162 - val_loss: 1.5431e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00015\n",
      "Epoch 53/450\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 3.8675e-04 - mean_absolute_error: 0.0161 - val_loss: 1.4656e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00015 to 0.00015, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 54/450\n",
      "76/76 [==============================] - 25s 331ms/step - loss: 3.5367e-04 - mean_absolute_error: 0.0157 - val_loss: 1.6492e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00015\n",
      "Epoch 55/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.8288e-04 - mean_absolute_error: 0.0160 - val_loss: 1.3708e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00015 to 0.00014, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 56/450\n",
      "76/76 [==============================] - 25s 328ms/step - loss: 3.9948e-04 - mean_absolute_error: 0.0170 - val_loss: 1.5637e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00014\n",
      "Epoch 57/450\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 4.0299e-04 - mean_absolute_error: 0.0164 - val_loss: 1.6071e-04 - val_mean_absolute_error: 0.0111\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00014\n",
      "Epoch 58/450\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 3.7016e-04 - mean_absolute_error: 0.0160 - val_loss: 1.5043e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00014\n",
      "Epoch 59/450\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 3.8731e-04 - mean_absolute_error: 0.0161 - val_loss: 3.1401e-04 - val_mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00014\n",
      "Epoch 60/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.8799e-04 - mean_absolute_error: 0.0161 - val_loss: 1.4406e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00014\n",
      "Epoch 61/450\n",
      "76/76 [==============================] - 24s 317ms/step - loss: 3.8275e-04 - mean_absolute_error: 0.0162 - val_loss: 1.6456e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00014\n",
      "Epoch 62/450\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 3.9402e-04 - mean_absolute_error: 0.0162 - val_loss: 1.6481e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00014\n",
      "Epoch 63/450\n",
      "76/76 [==============================] - 24s 316ms/step - loss: 3.7488e-04 - mean_absolute_error: 0.0163 - val_loss: 1.5627e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00014\n",
      "Epoch 64/450\n",
      "76/76 [==============================] - 24s 310ms/step - loss: 3.8016e-04 - mean_absolute_error: 0.0159 - val_loss: 2.3780e-04 - val_mean_absolute_error: 0.0109\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00014\n",
      "Epoch 65/450\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 3.5867e-04 - mean_absolute_error: 0.0159 - val_loss: 1.4560e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00014\n",
      "Epoch 66/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.5973e-04 - mean_absolute_error: 0.0159 - val_loss: 1.6596e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00014\n",
      "Epoch 67/450\n",
      "76/76 [==============================] - 22s 289ms/step - loss: 3.7183e-04 - mean_absolute_error: 0.0160 - val_loss: 1.8911e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00014\n",
      "Epoch 68/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 3.4996e-04 - mean_absolute_error: 0.0155 - val_loss: 2.2313e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00014\n",
      "Epoch 69/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 25s 324ms/step - loss: 3.8366e-04 - mean_absolute_error: 0.0163 - val_loss: 1.7143e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00014\n",
      "Epoch 70/450\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 3.8476e-04 - mean_absolute_error: 0.0163 - val_loss: 1.4208e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00014\n",
      "Epoch 71/450\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 3.8445e-04 - mean_absolute_error: 0.0162 - val_loss: 1.3065e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00014 to 0.00013, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 72/450\n",
      "76/76 [==============================] - 25s 327ms/step - loss: 3.5601e-04 - mean_absolute_error: 0.0158 - val_loss: 1.2951e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00013 to 0.00013, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 73/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.5706e-04 - mean_absolute_error: 0.0157 - val_loss: 1.8435e-04 - val_mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00013\n",
      "Epoch 74/450\n",
      "76/76 [==============================] - 24s 322ms/step - loss: 4.0786e-04 - mean_absolute_error: 0.0164 - val_loss: 1.6234e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00013\n",
      "Epoch 75/450\n",
      "76/76 [==============================] - 24s 316ms/step - loss: 3.5276e-04 - mean_absolute_error: 0.0161 - val_loss: 1.3379e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00013\n",
      "Epoch 76/450\n",
      "76/76 [==============================] - 25s 334ms/step - loss: 3.9188e-04 - mean_absolute_error: 0.0163 - val_loss: 1.3819e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00013\n",
      "Epoch 77/450\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 3.7703e-04 - mean_absolute_error: 0.0157 - val_loss: 1.5569e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00013\n",
      "Epoch 78/450\n",
      "76/76 [==============================] - 26s 337ms/step - loss: 4.1040e-04 - mean_absolute_error: 0.0166 - val_loss: 1.8274e-04 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00013\n",
      "Epoch 79/450\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 3.5927e-04 - mean_absolute_error: 0.0158 - val_loss: 2.6109e-04 - val_mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00013\n",
      "Epoch 80/450\n",
      "76/76 [==============================] - 24s 310ms/step - loss: 3.7085e-04 - mean_absolute_error: 0.0162 - val_loss: 1.7642e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00013\n",
      "Epoch 81/450\n",
      "76/76 [==============================] - 24s 318ms/step - loss: 4.0143e-04 - mean_absolute_error: 0.0164 - val_loss: 1.4847e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00013\n",
      "Epoch 82/450\n",
      "76/76 [==============================] - 22s 291ms/step - loss: 4.1682e-04 - mean_absolute_error: 0.0167 - val_loss: 1.3152e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00013\n",
      "Epoch 83/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 3.6092e-04 - mean_absolute_error: 0.0160 - val_loss: 1.8111e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00013\n",
      "Epoch 84/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.8485e-04 - mean_absolute_error: 0.0162 - val_loss: 1.9837e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00013\n",
      "Epoch 85/450\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 3.2346e-04 - mean_absolute_error: 0.0153 - val_loss: 1.7858e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00013\n",
      "Epoch 86/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 3.7308e-04 - mean_absolute_error: 0.0161 - val_loss: 1.3379e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00013\n",
      "Epoch 87/450\n",
      "76/76 [==============================] - 26s 336ms/step - loss: 3.2910e-04 - mean_absolute_error: 0.0153 - val_loss: 1.9066e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00013\n",
      "Epoch 88/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.7538e-04 - mean_absolute_error: 0.0159 - val_loss: 1.6992e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00013\n",
      "Epoch 89/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.7512e-04 - mean_absolute_error: 0.0164 - val_loss: 2.0962e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00013\n",
      "Epoch 90/450\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 3.5298e-04 - mean_absolute_error: 0.0158 - val_loss: 1.9650e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00013\n",
      "Epoch 91/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 4.1338e-04 - mean_absolute_error: 0.0167 - val_loss: 1.5837e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00013\n",
      "Epoch 92/450\n",
      "76/76 [==============================] - 25s 326ms/step - loss: 3.6264e-04 - mean_absolute_error: 0.0161 - val_loss: 2.0102e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00013\n",
      "Epoch 93/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 3.7830e-04 - mean_absolute_error: 0.0161 - val_loss: 1.3228e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00013\n",
      "Epoch 94/450\n",
      "76/76 [==============================] - 25s 331ms/step - loss: 3.6270e-04 - mean_absolute_error: 0.0159 - val_loss: 1.4932e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00013\n",
      "Epoch 95/450\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 3.5547e-04 - mean_absolute_error: 0.0158 - val_loss: 1.6935e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00013\n",
      "Epoch 96/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.7645e-04 - mean_absolute_error: 0.0160 - val_loss: 1.2874e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.00013 to 0.00013, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 97/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.9068e-04 - mean_absolute_error: 0.0163 - val_loss: 1.9295e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00013\n",
      "Epoch 98/450\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 3.7331e-04 - mean_absolute_error: 0.0159 - val_loss: 1.6263e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00013\n",
      "Epoch 99/450\n",
      "76/76 [==============================] - 24s 316ms/step - loss: 3.5133e-04 - mean_absolute_error: 0.0157 - val_loss: 1.4349e-04 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00013\n",
      "Epoch 100/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 3.6746e-04 - mean_absolute_error: 0.0156 - val_loss: 1.7690e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00013\n",
      "Epoch 101/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.3974e-04 - mean_absolute_error: 0.0156 - val_loss: 2.0661e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00013\n",
      "Epoch 102/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 3.6299e-04 - mean_absolute_error: 0.0160 - val_loss: 1.5662e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00013\n",
      "Epoch 103/450\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 3.7101e-04 - mean_absolute_error: 0.0156 - val_loss: 1.3147e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00013\n",
      "Epoch 104/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 24s 316ms/step - loss: 3.4873e-04 - mean_absolute_error: 0.0159 - val_loss: 1.4706e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00013\n",
      "Epoch 105/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 4.2099e-04 - mean_absolute_error: 0.0163 - val_loss: 1.7491e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00013\n",
      "Epoch 106/450\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 3.3896e-04 - mean_absolute_error: 0.0154 - val_loss: 1.6755e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00013\n",
      "Epoch 107/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 3.3486e-04 - mean_absolute_error: 0.0154 - val_loss: 1.6837e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00013\n",
      "Epoch 108/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 3.5304e-04 - mean_absolute_error: 0.0158 - val_loss: 1.5534e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00013\n",
      "Epoch 109/450\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 3.2815e-04 - mean_absolute_error: 0.0152 - val_loss: 1.4505e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00013\n",
      "Epoch 110/450\n",
      "76/76 [==============================] - 25s 324ms/step - loss: 3.6325e-04 - mean_absolute_error: 0.0159 - val_loss: 2.0339e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00013\n",
      "Epoch 111/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.3981e-04 - mean_absolute_error: 0.0153 - val_loss: 1.3571e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00013\n",
      "Epoch 112/450\n",
      "76/76 [==============================] - 24s 318ms/step - loss: 3.4844e-04 - mean_absolute_error: 0.0155 - val_loss: 1.3601e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00013\n",
      "Epoch 113/450\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 3.5715e-04 - mean_absolute_error: 0.0158 - val_loss: 1.9706e-04 - val_mean_absolute_error: 0.0107\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00013\n",
      "Epoch 114/450\n",
      "76/76 [==============================] - 24s 317ms/step - loss: 3.2744e-04 - mean_absolute_error: 0.0150 - val_loss: 1.5422e-04 - val_mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00013\n",
      "Epoch 115/450\n",
      "76/76 [==============================] - 25s 332ms/step - loss: 3.2449e-04 - mean_absolute_error: 0.0152 - val_loss: 1.6295e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00013\n",
      "Epoch 116/450\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 3.3615e-04 - mean_absolute_error: 0.0154 - val_loss: 1.4676e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00013\n",
      "Epoch 117/450\n",
      "76/76 [==============================] - 25s 322ms/step - loss: 3.4831e-04 - mean_absolute_error: 0.0155 - val_loss: 1.8455e-04 - val_mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00013\n",
      "Epoch 118/450\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 3.1368e-04 - mean_absolute_error: 0.0151 - val_loss: 1.3644e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00013\n",
      "Epoch 119/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 3.4604e-04 - mean_absolute_error: 0.0151 - val_loss: 1.5266e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00013\n",
      "Epoch 120/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 3.6279e-04 - mean_absolute_error: 0.0155 - val_loss: 1.6248e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00013\n",
      "Epoch 121/450\n",
      "76/76 [==============================] - 24s 309ms/step - loss: 3.5973e-04 - mean_absolute_error: 0.0155 - val_loss: 1.4101e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00013\n",
      "Epoch 122/450\n",
      "76/76 [==============================] - 25s 329ms/step - loss: 3.4854e-04 - mean_absolute_error: 0.0155 - val_loss: 1.3705e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00013\n",
      "Epoch 123/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 3.6259e-04 - mean_absolute_error: 0.0157 - val_loss: 1.4790e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00013\n",
      "Epoch 124/450\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 3.8243e-04 - mean_absolute_error: 0.0162 - val_loss: 1.3928e-04 - val_mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00013\n",
      "Epoch 125/450\n",
      "76/76 [==============================] - 25s 328ms/step - loss: 3.5616e-04 - mean_absolute_error: 0.0158 - val_loss: 1.6421e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00013\n",
      "Epoch 126/450\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 3.2593e-04 - mean_absolute_error: 0.0152 - val_loss: 1.7710e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00013\n",
      "Epoch 127/450\n",
      "76/76 [==============================] - 26s 344ms/step - loss: 3.7599e-04 - mean_absolute_error: 0.0158 - val_loss: 1.6838e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00013\n",
      "Epoch 128/450\n",
      "76/76 [==============================] - 25s 330ms/step - loss: 3.6569e-04 - mean_absolute_error: 0.0156 - val_loss: 1.9294e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00013\n",
      "Epoch 129/450\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 3.6806e-04 - mean_absolute_error: 0.0161 - val_loss: 1.8297e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00013\n",
      "Epoch 130/450\n",
      "76/76 [==============================] - 26s 337ms/step - loss: 3.6323e-04 - mean_absolute_error: 0.0155 - val_loss: 1.4550e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00013\n",
      "Epoch 131/450\n",
      "76/76 [==============================] - 24s 317ms/step - loss: 3.3582e-04 - mean_absolute_error: 0.0151 - val_loss: 1.3843e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00013\n",
      "Epoch 132/450\n",
      "76/76 [==============================] - 25s 327ms/step - loss: 3.3765e-04 - mean_absolute_error: 0.0153 - val_loss: 1.6612e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00013\n",
      "Epoch 133/450\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 3.5357e-04 - mean_absolute_error: 0.0152 - val_loss: 1.4658e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00013\n",
      "Epoch 134/450\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 3.6461e-04 - mean_absolute_error: 0.0156 - val_loss: 1.3330e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00013\n",
      "Epoch 135/450\n",
      "76/76 [==============================] - 22s 291ms/step - loss: 3.4939e-04 - mean_absolute_error: 0.0154 - val_loss: 1.9627e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00013\n",
      "Epoch 136/450\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 3.6436e-04 - mean_absolute_error: 0.0159 - val_loss: 1.6686e-04 - val_mean_absolute_error: 0.0103\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00013\n",
      "Epoch 137/450\n",
      "76/76 [==============================] - 24s 317ms/step - loss: 3.4698e-04 - mean_absolute_error: 0.0155 - val_loss: 1.3053e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00013\n",
      "Epoch 138/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.2063e-04 - mean_absolute_error: 0.0148 - val_loss: 1.5318e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00013\n",
      "Epoch 139/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.2765e-04 - mean_absolute_error: 0.0150 - val_loss: 1.5058e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00013\n",
      "Epoch 140/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 24s 316ms/step - loss: 3.4880e-04 - mean_absolute_error: 0.0156 - val_loss: 1.2980e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00013\n",
      "Epoch 141/450\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 3.3503e-04 - mean_absolute_error: 0.0151 - val_loss: 2.0139e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00013\n",
      "Epoch 142/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 3.6143e-04 - mean_absolute_error: 0.0155 - val_loss: 1.3819e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00013\n",
      "Epoch 143/450\n",
      "76/76 [==============================] - 24s 323ms/step - loss: 3.2437e-04 - mean_absolute_error: 0.0151 - val_loss: 1.4004e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00013\n",
      "Epoch 144/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.5477e-04 - mean_absolute_error: 0.0153 - val_loss: 2.1388e-04 - val_mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00013\n",
      "Epoch 145/450\n",
      "76/76 [==============================] - 24s 316ms/step - loss: 3.4508e-04 - mean_absolute_error: 0.0155 - val_loss: 1.5649e-04 - val_mean_absolute_error: 0.0114\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00013\n",
      "Epoch 146/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.0438e-04 - mean_absolute_error: 0.0149 - val_loss: 1.6515e-04 - val_mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00013\n",
      "Epoch 147/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.1524e-04 - mean_absolute_error: 0.0148 - val_loss: 1.5064e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00013\n",
      "Epoch 148/450\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 3.0889e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3810e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00013\n",
      "Epoch 149/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 3.3127e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3773e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00013\n",
      "Epoch 150/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 3.8126e-04 - mean_absolute_error: 0.0162 - val_loss: 1.3244e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00013\n",
      "Epoch 151/450\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 3.3638e-04 - mean_absolute_error: 0.0151 - val_loss: 1.3895e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00013\n",
      "Epoch 152/450\n",
      "76/76 [==============================] - 24s 311ms/step - loss: 3.7856e-04 - mean_absolute_error: 0.0160 - val_loss: 2.3060e-04 - val_mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00013\n",
      "Epoch 153/450\n",
      "76/76 [==============================] - 25s 330ms/step - loss: 3.7343e-04 - mean_absolute_error: 0.0159 - val_loss: 1.6686e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00013\n",
      "Epoch 154/450\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 3.6704e-04 - mean_absolute_error: 0.0157 - val_loss: 1.3048e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00013\n",
      "Epoch 155/450\n",
      "76/76 [==============================] - 25s 324ms/step - loss: 3.3883e-04 - mean_absolute_error: 0.0151 - val_loss: 1.3723e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00013\n",
      "Epoch 156/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 3.0764e-04 - mean_absolute_error: 0.0148 - val_loss: 2.4251e-04 - val_mean_absolute_error: 0.0117\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00013\n",
      "Epoch 157/450\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 3.3083e-04 - mean_absolute_error: 0.0149 - val_loss: 1.3243e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00013\n",
      "Epoch 158/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.3593e-04 - mean_absolute_error: 0.0150 - val_loss: 1.6574e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00013\n",
      "Epoch 159/450\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 3.1922e-04 - mean_absolute_error: 0.0151 - val_loss: 1.4145e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00013\n",
      "Epoch 160/450\n",
      "76/76 [==============================] - 24s 310ms/step - loss: 3.5953e-04 - mean_absolute_error: 0.0151 - val_loss: 1.2863e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00013 to 0.00013, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 161/450\n",
      "76/76 [==============================] - 25s 335ms/step - loss: 3.3864e-04 - mean_absolute_error: 0.0151 - val_loss: 1.5927e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00013\n",
      "Epoch 162/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.2585e-04 - mean_absolute_error: 0.0152 - val_loss: 1.3767e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00013\n",
      "Epoch 163/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 3.3693e-04 - mean_absolute_error: 0.0151 - val_loss: 1.5639e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00013\n",
      "Epoch 164/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 3.3139e-04 - mean_absolute_error: 0.0149 - val_loss: 1.2782e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.00013 to 0.00013, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 165/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 3.5058e-04 - mean_absolute_error: 0.0152 - val_loss: 1.9613e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00013\n",
      "Epoch 166/450\n",
      "76/76 [==============================] - 24s 322ms/step - loss: 3.4517e-04 - mean_absolute_error: 0.0152 - val_loss: 1.7743e-04 - val_mean_absolute_error: 0.0110\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00013\n",
      "Epoch 167/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.6161e-04 - mean_absolute_error: 0.0157 - val_loss: 1.4914e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00013\n",
      "Epoch 168/450\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 3.5502e-04 - mean_absolute_error: 0.0155 - val_loss: 1.3343e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00013\n",
      "Epoch 169/450\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 3.2665e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3318e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00013\n",
      "Epoch 170/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 3.1563e-04 - mean_absolute_error: 0.0149 - val_loss: 1.6519e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00013\n",
      "Epoch 171/450\n",
      "76/76 [==============================] - 26s 346ms/step - loss: 3.4124e-04 - mean_absolute_error: 0.0150 - val_loss: 1.2784e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00013\n",
      "Epoch 172/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 3.4768e-04 - mean_absolute_error: 0.0153 - val_loss: 1.2970e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00013\n",
      "Epoch 173/450\n",
      "76/76 [==============================] - 25s 332ms/step - loss: 3.3559e-04 - mean_absolute_error: 0.0152 - val_loss: 1.4275e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00013\n",
      "Epoch 174/450\n",
      "76/76 [==============================] - 25s 323ms/step - loss: 3.5411e-04 - mean_absolute_error: 0.0153 - val_loss: 1.3560e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00013\n",
      "Epoch 175/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 23s 308ms/step - loss: 3.2565e-04 - mean_absolute_error: 0.0153 - val_loss: 1.5206e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00013\n",
      "Epoch 176/450\n",
      "76/76 [==============================] - 26s 337ms/step - loss: 3.2389e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3720e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00013\n",
      "Epoch 177/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.3498e-04 - mean_absolute_error: 0.0152 - val_loss: 1.4059e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00013\n",
      "Epoch 178/450\n",
      "76/76 [==============================] - 25s 322ms/step - loss: 3.1977e-04 - mean_absolute_error: 0.0149 - val_loss: 1.3570e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00013\n",
      "Epoch 179/450\n",
      "76/76 [==============================] - 26s 345ms/step - loss: 3.5267e-04 - mean_absolute_error: 0.0153 - val_loss: 1.7880e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00013\n",
      "Epoch 180/450\n",
      "76/76 [==============================] - 24s 318ms/step - loss: 3.1518e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3246e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00013\n",
      "Epoch 181/450\n",
      "76/76 [==============================] - 26s 343ms/step - loss: 3.3875e-04 - mean_absolute_error: 0.0151 - val_loss: 2.2659e-04 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00013\n",
      "Epoch 182/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 3.4911e-04 - mean_absolute_error: 0.0154 - val_loss: 1.9236e-04 - val_mean_absolute_error: 0.0115\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00013\n",
      "Epoch 183/450\n",
      "76/76 [==============================] - 25s 328ms/step - loss: 3.2139e-04 - mean_absolute_error: 0.0150 - val_loss: 1.5889e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00013\n",
      "Epoch 184/450\n",
      "76/76 [==============================] - 23s 310ms/step - loss: 3.2544e-04 - mean_absolute_error: 0.0149 - val_loss: 1.4271e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00013\n",
      "Epoch 185/450\n",
      "76/76 [==============================] - 25s 328ms/step - loss: 3.4271e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3220e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00013\n",
      "Epoch 186/450\n",
      "76/76 [==============================] - 25s 330ms/step - loss: 3.4302e-04 - mean_absolute_error: 0.0154 - val_loss: 1.8635e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00013\n",
      "Epoch 187/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 3.2233e-04 - mean_absolute_error: 0.0149 - val_loss: 2.0471e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00013\n",
      "Epoch 188/450\n",
      "76/76 [==============================] - 25s 329ms/step - loss: 3.6752e-04 - mean_absolute_error: 0.0154 - val_loss: 1.4212e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00013\n",
      "Epoch 189/450\n",
      "76/76 [==============================] - 24s 306ms/step - loss: 3.4099e-04 - mean_absolute_error: 0.0152 - val_loss: 1.2735e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.00013 to 0.00013, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 190/450\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 3.0283e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3206e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00013\n",
      "Epoch 191/450\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 3.0809e-04 - mean_absolute_error: 0.0145 - val_loss: 1.2971e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00013\n",
      "Epoch 192/450\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 3.0915e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3289e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00013\n",
      "Epoch 193/450\n",
      "76/76 [==============================] - 25s 322ms/step - loss: 3.7611e-04 - mean_absolute_error: 0.0153 - val_loss: 1.5611e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00013\n",
      "Epoch 194/450\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 3.1568e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3799e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00013\n",
      "Epoch 195/450\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 3.1528e-04 - mean_absolute_error: 0.0148 - val_loss: 2.2176e-04 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00013\n",
      "Epoch 196/450\n",
      "76/76 [==============================] - 24s 311ms/step - loss: 3.2623e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3097e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00013\n",
      "Epoch 197/450\n",
      "76/76 [==============================] - 24s 316ms/step - loss: 3.2417e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3898e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00013\n",
      "Epoch 198/450\n",
      "76/76 [==============================] - 26s 346ms/step - loss: 3.3424e-04 - mean_absolute_error: 0.0151 - val_loss: 1.6651e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00013\n",
      "Epoch 199/450\n",
      "76/76 [==============================] - 24s 310ms/step - loss: 2.9929e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3095e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00013\n",
      "Epoch 200/450\n",
      "76/76 [==============================] - 24s 322ms/step - loss: 3.4883e-04 - mean_absolute_error: 0.0152 - val_loss: 1.5838e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00013\n",
      "Epoch 201/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.0753e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3185e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.00013\n",
      "Epoch 202/450\n",
      "76/76 [==============================] - 24s 318ms/step - loss: 3.3616e-04 - mean_absolute_error: 0.0151 - val_loss: 1.3720e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.00013\n",
      "Epoch 203/450\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 3.0791e-04 - mean_absolute_error: 0.0148 - val_loss: 1.6650e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.00013\n",
      "Epoch 204/450\n",
      "76/76 [==============================] - 26s 338ms/step - loss: 3.3740e-04 - mean_absolute_error: 0.0152 - val_loss: 1.7815e-04 - val_mean_absolute_error: 0.0113\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.00013\n",
      "Epoch 205/450\n",
      "76/76 [==============================] - 24s 322ms/step - loss: 3.5409e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3277e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.00013\n",
      "Epoch 206/450\n",
      "76/76 [==============================] - 25s 330ms/step - loss: 3.2304e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3865e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.00013\n",
      "Epoch 207/450\n",
      "76/76 [==============================] - 24s 311ms/step - loss: 3.3242e-04 - mean_absolute_error: 0.0152 - val_loss: 1.2934e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.00013\n",
      "Epoch 208/450\n",
      "76/76 [==============================] - 25s 332ms/step - loss: 3.4112e-04 - mean_absolute_error: 0.0151 - val_loss: 1.2992e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.00013\n",
      "Epoch 209/450\n",
      "76/76 [==============================] - 24s 318ms/step - loss: 3.3749e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3767e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.00013\n",
      "Epoch 210/450\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 3.3179e-04 - mean_absolute_error: 0.0148 - val_loss: 1.4226e-04 - val_mean_absolute_error: 0.0080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00210: val_loss did not improve from 0.00013\n",
      "Epoch 211/450\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 3.1838e-04 - mean_absolute_error: 0.0149 - val_loss: 1.2903e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.00013\n",
      "Epoch 212/450\n",
      "76/76 [==============================] - 25s 334ms/step - loss: 3.4741e-04 - mean_absolute_error: 0.0154 - val_loss: 1.4211e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.00013\n",
      "Epoch 213/450\n",
      "76/76 [==============================] - 25s 334ms/step - loss: 3.2855e-04 - mean_absolute_error: 0.0148 - val_loss: 1.2612e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00213: val_loss improved from 0.00013 to 0.00013, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 214/450\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 3.0249e-04 - mean_absolute_error: 0.0144 - val_loss: 1.6111e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.00013\n",
      "Epoch 215/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.3246e-04 - mean_absolute_error: 0.0150 - val_loss: 1.4394e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.00013\n",
      "Epoch 216/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 3.3226e-04 - mean_absolute_error: 0.0150 - val_loss: 1.5827e-04 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.00013\n",
      "Epoch 217/450\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 3.0978e-04 - mean_absolute_error: 0.0146 - val_loss: 1.4798e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.00013\n",
      "Epoch 218/450\n",
      "76/76 [==============================] - 25s 335ms/step - loss: 3.2328e-04 - mean_absolute_error: 0.0146 - val_loss: 1.5613e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.00013\n",
      "Epoch 219/450\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 3.0287e-04 - mean_absolute_error: 0.0146 - val_loss: 2.0155e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.00013\n",
      "Epoch 220/450\n",
      "76/76 [==============================] - 25s 326ms/step - loss: 3.2130e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3787e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.00013\n",
      "Epoch 221/450\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 3.1335e-04 - mean_absolute_error: 0.0147 - val_loss: 1.4506e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.00013\n",
      "Epoch 222/450\n",
      "76/76 [==============================] - 24s 311ms/step - loss: 2.9088e-04 - mean_absolute_error: 0.0143 - val_loss: 1.3945e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.00013\n",
      "Epoch 223/450\n",
      "76/76 [==============================] - 25s 331ms/step - loss: 3.2462e-04 - mean_absolute_error: 0.0148 - val_loss: 1.4179e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.00013\n",
      "Epoch 224/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.2877e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3263e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.00013\n",
      "Epoch 225/450\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 3.1204e-04 - mean_absolute_error: 0.0145 - val_loss: 1.4910e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.00013\n",
      "Epoch 226/450\n",
      "76/76 [==============================] - 25s 335ms/step - loss: 3.2567e-04 - mean_absolute_error: 0.0149 - val_loss: 1.3102e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.00013\n",
      "Epoch 227/450\n",
      "76/76 [==============================] - 24s 311ms/step - loss: 3.1124e-04 - mean_absolute_error: 0.0145 - val_loss: 2.1036e-04 - val_mean_absolute_error: 0.0116\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.00013\n",
      "Epoch 228/450\n",
      "76/76 [==============================] - 25s 335ms/step - loss: 3.3109e-04 - mean_absolute_error: 0.0151 - val_loss: 1.5106e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.00013\n",
      "Epoch 229/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 3.3081e-04 - mean_absolute_error: 0.0147 - val_loss: 1.4028e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.00013\n",
      "Epoch 230/450\n",
      "76/76 [==============================] - 26s 337ms/step - loss: 3.0119e-04 - mean_absolute_error: 0.0143 - val_loss: 1.4382e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.00013\n",
      "Epoch 231/450\n",
      "76/76 [==============================] - 28s 364ms/step - loss: 3.5577e-04 - mean_absolute_error: 0.0152 - val_loss: 1.6461e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.00013\n",
      "Epoch 232/450\n",
      "76/76 [==============================] - 25s 330ms/step - loss: 3.3237e-04 - mean_absolute_error: 0.0149 - val_loss: 1.5282e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.00013\n",
      "Epoch 233/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.1712e-04 - mean_absolute_error: 0.0148 - val_loss: 1.6077e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.00013\n",
      "Epoch 234/450\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 3.3189e-04 - mean_absolute_error: 0.0148 - val_loss: 1.8247e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.00013\n",
      "Epoch 235/450\n",
      "76/76 [==============================] - 25s 327ms/step - loss: 3.5053e-04 - mean_absolute_error: 0.0151 - val_loss: 1.3268e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.00013\n",
      "Epoch 236/450\n",
      "76/76 [==============================] - 25s 331ms/step - loss: 3.3758e-04 - mean_absolute_error: 0.0150 - val_loss: 1.2909e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.00013\n",
      "Epoch 237/450\n",
      "76/76 [==============================] - 25s 333ms/step - loss: 3.2143e-04 - mean_absolute_error: 0.0147 - val_loss: 1.7333e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.00013\n",
      "Epoch 238/450\n",
      "76/76 [==============================] - 26s 341ms/step - loss: 3.2820e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3135e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.00013\n",
      "Epoch 239/450\n",
      "76/76 [==============================] - 24s 309ms/step - loss: 2.8930e-04 - mean_absolute_error: 0.0144 - val_loss: 1.3786e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.00013\n",
      "Epoch 240/450\n",
      "76/76 [==============================] - 26s 339ms/step - loss: 3.4674e-04 - mean_absolute_error: 0.0151 - val_loss: 1.2668e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.00013\n",
      "Epoch 241/450\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 3.1749e-04 - mean_absolute_error: 0.0147 - val_loss: 2.3504e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.00013\n",
      "Epoch 242/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.1057e-04 - mean_absolute_error: 0.0146 - val_loss: 1.9509e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.00013\n",
      "Epoch 243/450\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 3.0203e-04 - mean_absolute_error: 0.0144 - val_loss: 1.6280e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.00013\n",
      "Epoch 244/450\n",
      "76/76 [==============================] - 22s 288ms/step - loss: 3.3872e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3510e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.00013\n",
      "Epoch 245/450\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 3.1208e-04 - mean_absolute_error: 0.0145 - val_loss: 1.2841e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.00013\n",
      "Epoch 246/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 25s 328ms/step - loss: 3.0313e-04 - mean_absolute_error: 0.0147 - val_loss: 1.2997e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.00013\n",
      "Epoch 247/450\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 3.3168e-04 - mean_absolute_error: 0.0149 - val_loss: 1.5600e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.00013\n",
      "Epoch 248/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.1035e-04 - mean_absolute_error: 0.0144 - val_loss: 1.6910e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.00013\n",
      "Epoch 249/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.0457e-04 - mean_absolute_error: 0.0143 - val_loss: 1.4597e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.00013\n",
      "Epoch 250/450\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 3.0505e-04 - mean_absolute_error: 0.0141 - val_loss: 1.3316e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.00013\n",
      "Epoch 251/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 3.3132e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3243e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.00013\n",
      "Epoch 252/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 3.6536e-04 - mean_absolute_error: 0.0152 - val_loss: 1.2718e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.00013\n",
      "Epoch 253/450\n",
      "76/76 [==============================] - 25s 331ms/step - loss: 3.3511e-04 - mean_absolute_error: 0.0148 - val_loss: 1.7495e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.00013\n",
      "Epoch 254/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.1763e-04 - mean_absolute_error: 0.0148 - val_loss: 1.6039e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.00013\n",
      "Epoch 255/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 3.1285e-04 - mean_absolute_error: 0.0148 - val_loss: 1.2421e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.00013 to 0.00012, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 256/450\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 2.8839e-04 - mean_absolute_error: 0.0141 - val_loss: 1.2689e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.00012\n",
      "Epoch 257/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.1724e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3531e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.00012\n",
      "Epoch 258/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.2528e-04 - mean_absolute_error: 0.0148 - val_loss: 1.4206e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.00012\n",
      "Epoch 259/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.2488e-04 - mean_absolute_error: 0.0148 - val_loss: 1.8373e-04 - val_mean_absolute_error: 0.0098\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.00012\n",
      "Epoch 260/450\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 3.1626e-04 - mean_absolute_error: 0.0145 - val_loss: 1.7764e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.00012\n",
      "Epoch 261/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 3.3767e-04 - mean_absolute_error: 0.0149 - val_loss: 1.7281e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.00012\n",
      "Epoch 262/450\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 3.3444e-04 - mean_absolute_error: 0.0148 - val_loss: 1.8669e-04 - val_mean_absolute_error: 0.0096\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.00012\n",
      "Epoch 263/450\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 3.3839e-04 - mean_absolute_error: 0.0149 - val_loss: 2.6581e-04 - val_mean_absolute_error: 0.0124\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.00012\n",
      "Epoch 264/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.1428e-04 - mean_absolute_error: 0.0145 - val_loss: 1.8377e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.00012\n",
      "Epoch 265/450\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 3.2111e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3157e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.00012\n",
      "Epoch 266/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.4271e-04 - mean_absolute_error: 0.0151 - val_loss: 1.6786e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.00012\n",
      "Epoch 267/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.1704e-04 - mean_absolute_error: 0.0144 - val_loss: 1.4460e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.00012\n",
      "Epoch 268/450\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 3.3587e-04 - mean_absolute_error: 0.0148 - val_loss: 1.6176e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.00012\n",
      "Epoch 269/450\n",
      "76/76 [==============================] - 25s 333ms/step - loss: 3.3102e-04 - mean_absolute_error: 0.0150 - val_loss: 1.3767e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.00012\n",
      "Epoch 270/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 3.1054e-04 - mean_absolute_error: 0.0144 - val_loss: 1.6678e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.00012\n",
      "Epoch 271/450\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 3.2821e-04 - mean_absolute_error: 0.0149 - val_loss: 1.4237e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.00012\n",
      "Epoch 272/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.3383e-04 - mean_absolute_error: 0.0147 - val_loss: 1.4094e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.00012\n",
      "Epoch 273/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.3476e-04 - mean_absolute_error: 0.0152 - val_loss: 1.3023e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.00012\n",
      "Epoch 274/450\n",
      "76/76 [==============================] - 25s 332ms/step - loss: 3.1070e-04 - mean_absolute_error: 0.0145 - val_loss: 1.4597e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.00012\n",
      "Epoch 275/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 3.2590e-04 - mean_absolute_error: 0.0147 - val_loss: 1.4755e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.00012\n",
      "Epoch 276/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 3.3467e-04 - mean_absolute_error: 0.0148 - val_loss: 1.2461e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.00012\n",
      "Epoch 277/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.2977e-04 - mean_absolute_error: 0.0149 - val_loss: 1.3119e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.00012\n",
      "Epoch 278/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 2.9941e-04 - mean_absolute_error: 0.0143 - val_loss: 1.6407e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.00012\n",
      "Epoch 279/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 3.0656e-04 - mean_absolute_error: 0.0146 - val_loss: 1.2887e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.00012\n",
      "Epoch 280/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 2.9150e-04 - mean_absolute_error: 0.0141 - val_loss: 1.3986e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.00012\n",
      "Epoch 281/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 2.9484e-04 - mean_absolute_error: 0.0143 - val_loss: 1.4100e-04 - val_mean_absolute_error: 0.0083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00281: val_loss did not improve from 0.00012\n",
      "Epoch 282/450\n",
      "76/76 [==============================] - 22s 285ms/step - loss: 2.9137e-04 - mean_absolute_error: 0.0141 - val_loss: 1.3284e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.00012\n",
      "Epoch 283/450\n",
      "76/76 [==============================] - 22s 285ms/step - loss: 3.0351e-04 - mean_absolute_error: 0.0140 - val_loss: 1.7616e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.00012\n",
      "Epoch 284/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 3.2320e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3637e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.00012\n",
      "Epoch 285/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.5050e-04 - mean_absolute_error: 0.0152 - val_loss: 1.2871e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.00012\n",
      "Epoch 286/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 3.0833e-04 - mean_absolute_error: 0.0146 - val_loss: 1.5220e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.00012\n",
      "Epoch 287/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.1222e-04 - mean_absolute_error: 0.0146 - val_loss: 1.2729e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.00012\n",
      "Epoch 288/450\n",
      "76/76 [==============================] - 21s 271ms/step - loss: 3.2527e-04 - mean_absolute_error: 0.0146 - val_loss: 1.2914e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.00012\n",
      "Epoch 289/450\n",
      "76/76 [==============================] - 21s 283ms/step - loss: 3.1851e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3321e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.00012\n",
      "Epoch 290/450\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 3.1613e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3545e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.00012\n",
      "Epoch 291/450\n",
      "76/76 [==============================] - 21s 277ms/step - loss: 3.1986e-04 - mean_absolute_error: 0.0145 - val_loss: 1.6795e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.00012\n",
      "Epoch 292/450\n",
      "76/76 [==============================] - 21s 283ms/step - loss: 3.3008e-04 - mean_absolute_error: 0.0151 - val_loss: 1.3314e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.00012\n",
      "Epoch 293/450\n",
      "76/76 [==============================] - 23s 302ms/step - loss: 2.9917e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3641e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.00012\n",
      "Epoch 294/450\n",
      "76/76 [==============================] - 21s 271ms/step - loss: 2.9983e-04 - mean_absolute_error: 0.0144 - val_loss: 1.3055e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.00012\n",
      "Epoch 295/450\n",
      "76/76 [==============================] - 22s 290ms/step - loss: 3.1752e-04 - mean_absolute_error: 0.0146 - val_loss: 1.4593e-04 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.00012\n",
      "Epoch 296/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.0159e-04 - mean_absolute_error: 0.0143 - val_loss: 1.2909e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.00012\n",
      "Epoch 297/450\n",
      "76/76 [==============================] - 21s 277ms/step - loss: 3.0456e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3705e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.00012\n",
      "Epoch 298/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.1976e-04 - mean_absolute_error: 0.0146 - val_loss: 1.2565e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.00012\n",
      "Epoch 299/450\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 3.0825e-04 - mean_absolute_error: 0.0144 - val_loss: 1.3824e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.00012\n",
      "Epoch 300/450\n",
      "76/76 [==============================] - 21s 274ms/step - loss: 3.3360e-04 - mean_absolute_error: 0.0149 - val_loss: 1.7989e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.00012\n",
      "Epoch 301/450\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 3.1843e-04 - mean_absolute_error: 0.0146 - val_loss: 1.5998e-04 - val_mean_absolute_error: 0.0106\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.00012\n",
      "Epoch 302/450\n",
      "76/76 [==============================] - 21s 283ms/step - loss: 3.0942e-04 - mean_absolute_error: 0.0141 - val_loss: 1.4456e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.00012\n",
      "Epoch 303/450\n",
      "76/76 [==============================] - 22s 284ms/step - loss: 3.2473e-04 - mean_absolute_error: 0.0147 - val_loss: 2.6318e-04 - val_mean_absolute_error: 0.0134\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.00012\n",
      "Epoch 304/450\n",
      "76/76 [==============================] - 21s 276ms/step - loss: 3.7464e-04 - mean_absolute_error: 0.0156 - val_loss: 1.4695e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.00012\n",
      "Epoch 305/450\n",
      "76/76 [==============================] - 20s 270ms/step - loss: 3.1392e-04 - mean_absolute_error: 0.0144 - val_loss: 1.4304e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.00012\n",
      "Epoch 306/450\n",
      "76/76 [==============================] - 21s 281ms/step - loss: 2.9832e-04 - mean_absolute_error: 0.0144 - val_loss: 1.4244e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.00012\n",
      "Epoch 307/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 3.1128e-04 - mean_absolute_error: 0.0148 - val_loss: 1.4547e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.00012\n",
      "Epoch 308/450\n",
      "76/76 [==============================] - 21s 271ms/step - loss: 3.3589e-04 - mean_absolute_error: 0.0148 - val_loss: 1.5200e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.00012\n",
      "Epoch 309/450\n",
      "76/76 [==============================] - 22s 288ms/step - loss: 3.0899e-04 - mean_absolute_error: 0.0144 - val_loss: 1.5038e-04 - val_mean_absolute_error: 0.0091\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.00012\n",
      "Epoch 310/450\n",
      "76/76 [==============================] - 21s 273ms/step - loss: 3.2983e-04 - mean_absolute_error: 0.0146 - val_loss: 1.9077e-04 - val_mean_absolute_error: 0.0090\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.00012\n",
      "Epoch 311/450\n",
      "76/76 [==============================] - 21s 271ms/step - loss: 3.1799e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3788e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.00012\n",
      "Epoch 312/450\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 3.3672e-04 - mean_absolute_error: 0.0146 - val_loss: 1.4724e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.00012\n",
      "Epoch 313/450\n",
      "76/76 [==============================] - 21s 282ms/step - loss: 3.1365e-04 - mean_absolute_error: 0.0146 - val_loss: 1.3347e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.00012\n",
      "Epoch 314/450\n",
      "76/76 [==============================] - 21s 275ms/step - loss: 3.1398e-04 - mean_absolute_error: 0.0147 - val_loss: 1.2476e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.00012\n",
      "Epoch 315/450\n",
      "76/76 [==============================] - 22s 287ms/step - loss: 3.2366e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3910e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.00012\n",
      "Epoch 316/450\n",
      "76/76 [==============================] - 21s 271ms/step - loss: 3.2185e-04 - mean_absolute_error: 0.0148 - val_loss: 1.4080e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.00012\n",
      "Epoch 317/450\n",
      "76/76 [==============================] - 21s 281ms/step - loss: 3.0701e-04 - mean_absolute_error: 0.0144 - val_loss: 1.3680e-04 - val_mean_absolute_error: 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00317: val_loss did not improve from 0.00012\n",
      "Epoch 318/450\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 2.9825e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3595e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.00012\n",
      "Epoch 319/450\n",
      "76/76 [==============================] - 20s 270ms/step - loss: 3.0070e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3294e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.00012\n",
      "Epoch 320/450\n",
      "76/76 [==============================] - 22s 285ms/step - loss: 3.2620e-04 - mean_absolute_error: 0.0148 - val_loss: 1.2911e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.00012\n",
      "Epoch 321/450\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 2.9399e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3266e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.00012\n",
      "Epoch 322/450\n",
      "76/76 [==============================] - 20s 269ms/step - loss: 3.1810e-04 - mean_absolute_error: 0.0145 - val_loss: 1.4744e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.00012\n",
      "Epoch 323/450\n",
      "76/76 [==============================] - 21s 283ms/step - loss: 3.2363e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3089e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.00012\n",
      "Epoch 324/450\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 2.9310e-04 - mean_absolute_error: 0.0141 - val_loss: 1.3470e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.00012\n",
      "Epoch 325/450\n",
      "76/76 [==============================] - 20s 270ms/step - loss: 3.2407e-04 - mean_absolute_error: 0.0145 - val_loss: 1.4949e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.00012\n",
      "Epoch 326/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.0096e-04 - mean_absolute_error: 0.0142 - val_loss: 1.4291e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.00012\n",
      "Epoch 327/450\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 3.4863e-04 - mean_absolute_error: 0.0151 - val_loss: 1.4270e-04 - val_mean_absolute_error: 0.0092\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.00012\n",
      "Epoch 328/450\n",
      "76/76 [==============================] - 28s 371ms/step - loss: 3.0183e-04 - mean_absolute_error: 0.0143 - val_loss: 1.3701e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.00012\n",
      "Epoch 329/450\n",
      "76/76 [==============================] - 28s 372ms/step - loss: 3.1854e-04 - mean_absolute_error: 0.0147 - val_loss: 1.4773e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.00012\n",
      "Epoch 330/450\n",
      "76/76 [==============================] - 25s 328ms/step - loss: 3.1620e-04 - mean_absolute_error: 0.0145 - val_loss: 1.2445e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.00012\n",
      "Epoch 331/450\n",
      "76/76 [==============================] - 27s 363ms/step - loss: 3.1726e-04 - mean_absolute_error: 0.0147 - val_loss: 1.2947e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.00012\n",
      "Epoch 332/450\n",
      "76/76 [==============================] - 27s 360ms/step - loss: 2.9893e-04 - mean_absolute_error: 0.0142 - val_loss: 1.4175e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.00012\n",
      "Epoch 333/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 2.9055e-04 - mean_absolute_error: 0.0142 - val_loss: 1.4088e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.00012\n",
      "Epoch 334/450\n",
      "76/76 [==============================] - 29s 378ms/step - loss: 3.1938e-04 - mean_absolute_error: 0.0146 - val_loss: 1.4589e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.00012\n",
      "Epoch 335/450\n",
      "76/76 [==============================] - 27s 352ms/step - loss: 3.1466e-04 - mean_absolute_error: 0.0144 - val_loss: 1.3123e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.00012\n",
      "Epoch 336/450\n",
      "76/76 [==============================] - 24s 318ms/step - loss: 2.9650e-04 - mean_absolute_error: 0.0143 - val_loss: 1.2570e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.00012\n",
      "Epoch 337/450\n",
      "76/76 [==============================] - 25s 331ms/step - loss: 2.9392e-04 - mean_absolute_error: 0.0139 - val_loss: 1.3279e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.00012\n",
      "Epoch 338/450\n",
      "76/76 [==============================] - 25s 332ms/step - loss: 3.3826e-04 - mean_absolute_error: 0.0148 - val_loss: 1.2935e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.00012\n",
      "Epoch 339/450\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 2.8725e-04 - mean_absolute_error: 0.0140 - val_loss: 1.3276e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.00012\n",
      "Epoch 340/450\n",
      "76/76 [==============================] - 26s 346ms/step - loss: 3.0689e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3971e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.00012\n",
      "Epoch 341/450\n",
      "76/76 [==============================] - 28s 371ms/step - loss: 3.2324e-04 - mean_absolute_error: 0.0147 - val_loss: 1.5961e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.00012\n",
      "Epoch 342/450\n",
      "76/76 [==============================] - 26s 344ms/step - loss: 2.8414e-04 - mean_absolute_error: 0.0141 - val_loss: 1.3454e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.00012\n",
      "Epoch 343/450\n",
      "76/76 [==============================] - 27s 353ms/step - loss: 3.0103e-04 - mean_absolute_error: 0.0142 - val_loss: 1.4276e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.00012\n",
      "Epoch 344/450\n",
      "76/76 [==============================] - 28s 367ms/step - loss: 3.1007e-04 - mean_absolute_error: 0.0143 - val_loss: 1.2299e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00344: val_loss improved from 0.00012 to 0.00012, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 345/450\n",
      "76/76 [==============================] - 25s 331ms/step - loss: 3.2428e-04 - mean_absolute_error: 0.0147 - val_loss: 1.6247e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.00012\n",
      "Epoch 346/450\n",
      "76/76 [==============================] - 25s 332ms/step - loss: 3.2052e-04 - mean_absolute_error: 0.0144 - val_loss: 1.2710e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.00012\n",
      "Epoch 347/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 3.4078e-04 - mean_absolute_error: 0.0149 - val_loss: 1.3589e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.00012\n",
      "Epoch 348/450\n",
      "76/76 [==============================] - 29s 385ms/step - loss: 2.9272e-04 - mean_absolute_error: 0.0140 - val_loss: 1.4368e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.00012\n",
      "Epoch 349/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.1419e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3050e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.00012\n",
      "Epoch 350/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 3.1103e-04 - mean_absolute_error: 0.0143 - val_loss: 2.0270e-04 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.00012\n",
      "Epoch 351/450\n",
      "76/76 [==============================] - 25s 326ms/step - loss: 3.0290e-04 - mean_absolute_error: 0.0144 - val_loss: 1.2595e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.00012\n",
      "Epoch 352/450\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 3.0660e-04 - mean_absolute_error: 0.0140 - val_loss: 1.3180e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.00012\n",
      "Epoch 353/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 22s 288ms/step - loss: 3.0546e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3753e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.00012\n",
      "Epoch 354/450\n",
      "76/76 [==============================] - 22s 285ms/step - loss: 3.1429e-04 - mean_absolute_error: 0.0145 - val_loss: 1.2607e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.00012\n",
      "Epoch 355/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 2.9669e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3031e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.00012\n",
      "Epoch 356/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.0736e-04 - mean_absolute_error: 0.0143 - val_loss: 1.5718e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.00012\n",
      "Epoch 357/450\n",
      "76/76 [==============================] - 24s 311ms/step - loss: 3.0771e-04 - mean_absolute_error: 0.0143 - val_loss: 1.2878e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.00012\n",
      "Epoch 358/450\n",
      "76/76 [==============================] - 25s 334ms/step - loss: 3.0834e-04 - mean_absolute_error: 0.0143 - val_loss: 1.7751e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.00012\n",
      "Epoch 359/450\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 3.0091e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3343e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.00012\n",
      "Epoch 360/450\n",
      "76/76 [==============================] - 25s 324ms/step - loss: 3.2777e-04 - mean_absolute_error: 0.0147 - val_loss: 1.4327e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.00012\n",
      "Epoch 361/450\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 3.1479e-04 - mean_absolute_error: 0.0144 - val_loss: 1.4731e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.00012\n",
      "Epoch 362/450\n",
      "76/76 [==============================] - 24s 310ms/step - loss: 3.2094e-04 - mean_absolute_error: 0.0146 - val_loss: 1.4488e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.00012\n",
      "Epoch 363/450\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 2.9963e-04 - mean_absolute_error: 0.0140 - val_loss: 1.4867e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.00012\n",
      "Epoch 364/450\n",
      "76/76 [==============================] - 22s 286ms/step - loss: 3.0412e-04 - mean_absolute_error: 0.0142 - val_loss: 1.7031e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.00012\n",
      "Epoch 365/450\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 3.4143e-04 - mean_absolute_error: 0.0149 - val_loss: 1.4663e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.00012\n",
      "Epoch 366/450\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 2.9094e-04 - mean_absolute_error: 0.0142 - val_loss: 1.2762e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.00012\n",
      "Epoch 367/450\n",
      "76/76 [==============================] - 21s 282ms/step - loss: 3.0989e-04 - mean_absolute_error: 0.0144 - val_loss: 1.2916e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.00012\n",
      "Epoch 368/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 3.0580e-04 - mean_absolute_error: 0.0140 - val_loss: 1.2856e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.00012\n",
      "Epoch 369/450\n",
      "76/76 [==============================] - 24s 311ms/step - loss: 3.1196e-04 - mean_absolute_error: 0.0143 - val_loss: 1.4329e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.00012\n",
      "Epoch 370/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 2.8693e-04 - mean_absolute_error: 0.0140 - val_loss: 1.3252e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.00012\n",
      "Epoch 371/450\n",
      "76/76 [==============================] - 25s 330ms/step - loss: 3.1656e-04 - mean_absolute_error: 0.0144 - val_loss: 1.8324e-04 - val_mean_absolute_error: 0.0102\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.00012\n",
      "Epoch 372/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.1172e-04 - mean_absolute_error: 0.0143 - val_loss: 1.8993e-04 - val_mean_absolute_error: 0.0112\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.00012\n",
      "Epoch 373/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.0129e-04 - mean_absolute_error: 0.0141 - val_loss: 1.5772e-04 - val_mean_absolute_error: 0.0094\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.00012\n",
      "Epoch 374/450\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 2.9343e-04 - mean_absolute_error: 0.0141 - val_loss: 1.5369e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.00012\n",
      "Epoch 375/450\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 3.0965e-04 - mean_absolute_error: 0.0141 - val_loss: 1.3601e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.00012\n",
      "Epoch 376/450\n",
      "76/76 [==============================] - 26s 338ms/step - loss: 3.1446e-04 - mean_absolute_error: 0.0142 - val_loss: 1.5880e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.00012\n",
      "Epoch 377/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 3.0155e-04 - mean_absolute_error: 0.0141 - val_loss: 1.4204e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.00012\n",
      "Epoch 378/450\n",
      "76/76 [==============================] - 27s 359ms/step - loss: 3.0294e-04 - mean_absolute_error: 0.0138 - val_loss: 1.2857e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.00012\n",
      "Epoch 379/450\n",
      "76/76 [==============================] - 26s 341ms/step - loss: 3.1447e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3490e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.00012\n",
      "Epoch 380/450\n",
      "76/76 [==============================] - 27s 357ms/step - loss: 2.9188e-04 - mean_absolute_error: 0.0139 - val_loss: 1.3313e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.00012\n",
      "Epoch 381/450\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 3.4940e-04 - mean_absolute_error: 0.0150 - val_loss: 1.5193e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.00012\n",
      "Epoch 382/450\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 3.6513e-04 - mean_absolute_error: 0.0156 - val_loss: 1.7365e-04 - val_mean_absolute_error: 0.0099\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.00012\n",
      "Epoch 383/450\n",
      "76/76 [==============================] - 25s 330ms/step - loss: 3.0398e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3603e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.00012\n",
      "Epoch 384/450\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 2.9547e-04 - mean_absolute_error: 0.0142 - val_loss: 1.4078e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.00012\n",
      "Epoch 385/450\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 2.8675e-04 - mean_absolute_error: 0.0141 - val_loss: 1.3048e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.00012\n",
      "Epoch 386/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 2.9493e-04 - mean_absolute_error: 0.0142 - val_loss: 1.2656e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.00012\n",
      "Epoch 387/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.1141e-04 - mean_absolute_error: 0.0142 - val_loss: 1.4496e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.00012\n",
      "Epoch 388/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 3.0994e-04 - mean_absolute_error: 0.0141 - val_loss: 2.2108e-04 - val_mean_absolute_error: 0.0101\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.00012\n",
      "Epoch 389/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 25s 332ms/step - loss: 3.1161e-04 - mean_absolute_error: 0.0145 - val_loss: 1.3275e-04 - val_mean_absolute_error: 0.0085\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.00012\n",
      "Epoch 390/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 2.9676e-04 - mean_absolute_error: 0.0139 - val_loss: 1.2625e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.00012\n",
      "Epoch 391/450\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 3.1288e-04 - mean_absolute_error: 0.0145 - val_loss: 1.2330e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.00012\n",
      "Epoch 392/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 3.0477e-04 - mean_absolute_error: 0.0141 - val_loss: 1.2469e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.00012\n",
      "Epoch 393/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 3.3587e-04 - mean_absolute_error: 0.0146 - val_loss: 1.6020e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.00012\n",
      "Epoch 394/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.2686e-04 - mean_absolute_error: 0.0146 - val_loss: 1.4702e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.00012\n",
      "Epoch 395/450\n",
      "76/76 [==============================] - 22s 295ms/step - loss: 3.1957e-04 - mean_absolute_error: 0.0147 - val_loss: 1.2700e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.00012\n",
      "Epoch 396/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 2.9320e-04 - mean_absolute_error: 0.0140 - val_loss: 1.4622e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.00012\n",
      "Epoch 397/450\n",
      "76/76 [==============================] - 22s 292ms/step - loss: 3.2160e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3660e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.00012\n",
      "Epoch 398/450\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 2.7703e-04 - mean_absolute_error: 0.0137 - val_loss: 1.2799e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.00012\n",
      "Epoch 399/450\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 3.1135e-04 - mean_absolute_error: 0.0142 - val_loss: 1.2234e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.00012 to 0.00012, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 400/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 3.3114e-04 - mean_absolute_error: 0.0147 - val_loss: 1.3135e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.00012\n",
      "Epoch 401/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 3.0281e-04 - mean_absolute_error: 0.0141 - val_loss: 1.2349e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.00012\n",
      "Epoch 402/450\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 2.9510e-04 - mean_absolute_error: 0.0141 - val_loss: 1.8288e-04 - val_mean_absolute_error: 0.0105\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.00012\n",
      "Epoch 403/450\n",
      "76/76 [==============================] - 22s 288ms/step - loss: 3.3314e-04 - mean_absolute_error: 0.0148 - val_loss: 1.3589e-04 - val_mean_absolute_error: 0.0097\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.00012\n",
      "Epoch 404/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.0532e-04 - mean_absolute_error: 0.0142 - val_loss: 1.4874e-04 - val_mean_absolute_error: 0.0086\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.00012\n",
      "Epoch 405/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.0687e-04 - mean_absolute_error: 0.0144 - val_loss: 1.2541e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.00012\n",
      "Epoch 406/450\n",
      "76/76 [==============================] - 27s 353ms/step - loss: 3.0388e-04 - mean_absolute_error: 0.0143 - val_loss: 1.3009e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.00012\n",
      "Epoch 407/450\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 3.1356e-04 - mean_absolute_error: 0.0142 - val_loss: 1.4157e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.00012\n",
      "Epoch 408/450\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 2.9767e-04 - mean_absolute_error: 0.0142 - val_loss: 1.5495e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.00012\n",
      "Epoch 409/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.0382e-04 - mean_absolute_error: 0.0141 - val_loss: 1.6404e-04 - val_mean_absolute_error: 0.0095\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.00012\n",
      "Epoch 410/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 2.8754e-04 - mean_absolute_error: 0.0140 - val_loss: 1.5601e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.00012\n",
      "Epoch 411/450\n",
      "76/76 [==============================] - 23s 304ms/step - loss: 3.4513e-04 - mean_absolute_error: 0.0151 - val_loss: 1.3082e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.00012\n",
      "Epoch 412/450\n",
      "76/76 [==============================] - 24s 322ms/step - loss: 2.9908e-04 - mean_absolute_error: 0.0141 - val_loss: 1.4013e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.00012\n",
      "Epoch 413/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.0373e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3753e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.00012\n",
      "Epoch 414/450\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 2.9097e-04 - mean_absolute_error: 0.0140 - val_loss: 1.2178e-04 - val_mean_absolute_error: 0.0071\n",
      "\n",
      "Epoch 00414: val_loss improved from 0.00012 to 0.00012, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 415/450\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 3.2503e-04 - mean_absolute_error: 0.0146 - val_loss: 1.3371e-04 - val_mean_absolute_error: 0.0081\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.00012\n",
      "Epoch 416/450\n",
      "76/76 [==============================] - 22s 288ms/step - loss: 3.0770e-04 - mean_absolute_error: 0.0141 - val_loss: 1.4142e-04 - val_mean_absolute_error: 0.0083\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.00012\n",
      "Epoch 417/450\n",
      "76/76 [==============================] - 24s 317ms/step - loss: 2.9543e-04 - mean_absolute_error: 0.0142 - val_loss: 1.5250e-04 - val_mean_absolute_error: 0.0087\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.00012\n",
      "Epoch 418/450\n",
      "76/76 [==============================] - 25s 324ms/step - loss: 3.1922e-04 - mean_absolute_error: 0.0145 - val_loss: 1.2738e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.00012\n",
      "Epoch 419/450\n",
      "76/76 [==============================] - 23s 303ms/step - loss: 3.1336e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3729e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.00012\n",
      "Epoch 420/450\n",
      "76/76 [==============================] - 25s 323ms/step - loss: 2.9465e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3177e-04 - val_mean_absolute_error: 0.0076\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.00012\n",
      "Epoch 421/450\n",
      "76/76 [==============================] - 23s 297ms/step - loss: 3.3962e-04 - mean_absolute_error: 0.0147 - val_loss: 1.2642e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.00012\n",
      "Epoch 422/450\n",
      "76/76 [==============================] - 25s 324ms/step - loss: 2.8764e-04 - mean_absolute_error: 0.0140 - val_loss: 1.2638e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.00012\n",
      "Epoch 423/450\n",
      "76/76 [==============================] - 23s 309ms/step - loss: 3.1619e-04 - mean_absolute_error: 0.0143 - val_loss: 1.3347e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.00012\n",
      "Epoch 424/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 23s 304ms/step - loss: 3.0326e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3473e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.00012\n",
      "Epoch 425/450\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 2.8689e-04 - mean_absolute_error: 0.0138 - val_loss: 1.2798e-04 - val_mean_absolute_error: 0.0082\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.00012\n",
      "Epoch 426/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 3.2201e-04 - mean_absolute_error: 0.0143 - val_loss: 1.2555e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.00012\n",
      "Epoch 427/450\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 2.8453e-04 - mean_absolute_error: 0.0140 - val_loss: 1.2081e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00427: val_loss improved from 0.00012 to 0.00012, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n",
      "Epoch 428/450\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 2.9416e-04 - mean_absolute_error: 0.0138 - val_loss: 1.2932e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.00012\n",
      "Epoch 429/450\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 2.8998e-04 - mean_absolute_error: 0.0140 - val_loss: 1.3105e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.00012\n",
      "Epoch 430/450\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 3.2479e-04 - mean_absolute_error: 0.0142 - val_loss: 1.6486e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.00012\n",
      "Epoch 431/450\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 3.2041e-04 - mean_absolute_error: 0.0146 - val_loss: 1.4547e-04 - val_mean_absolute_error: 0.0093\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.00012\n",
      "Epoch 432/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 3.0816e-04 - mean_absolute_error: 0.0143 - val_loss: 1.3240e-04 - val_mean_absolute_error: 0.0079\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.00012\n",
      "Epoch 433/450\n",
      "76/76 [==============================] - 24s 322ms/step - loss: 3.0105e-04 - mean_absolute_error: 0.0143 - val_loss: 1.2201e-04 - val_mean_absolute_error: 0.0073\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.00012\n",
      "Epoch 434/450\n",
      "76/76 [==============================] - 22s 296ms/step - loss: 3.3286e-04 - mean_absolute_error: 0.0144 - val_loss: 1.2573e-04 - val_mean_absolute_error: 0.0077\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.00012\n",
      "Epoch 435/450\n",
      "76/76 [==============================] - 24s 323ms/step - loss: 3.1316e-04 - mean_absolute_error: 0.0143 - val_loss: 1.4600e-04 - val_mean_absolute_error: 0.0088\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.00012\n",
      "Epoch 436/450\n",
      "76/76 [==============================] - 25s 329ms/step - loss: 3.0613e-04 - mean_absolute_error: 0.0141 - val_loss: 1.2570e-04 - val_mean_absolute_error: 0.0080\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.00012\n",
      "Epoch 437/450\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 2.9346e-04 - mean_absolute_error: 0.0140 - val_loss: 1.3161e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.00012\n",
      "Epoch 438/450\n",
      "76/76 [==============================] - 25s 335ms/step - loss: 3.1225e-04 - mean_absolute_error: 0.0144 - val_loss: 1.2626e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.00012\n",
      "Epoch 439/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 2.9288e-04 - mean_absolute_error: 0.0141 - val_loss: 1.6636e-04 - val_mean_absolute_error: 0.0089\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.00012\n",
      "Epoch 440/450\n",
      "76/76 [==============================] - 27s 362ms/step - loss: 3.3920e-04 - mean_absolute_error: 0.0147 - val_loss: 1.4755e-04 - val_mean_absolute_error: 0.0084\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.00012\n",
      "Epoch 441/450\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 2.8605e-04 - mean_absolute_error: 0.0141 - val_loss: 1.2806e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.00012\n",
      "Epoch 442/450\n",
      "76/76 [==============================] - 23s 299ms/step - loss: 2.9826e-04 - mean_absolute_error: 0.0142 - val_loss: 1.3124e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.00012\n",
      "Epoch 443/450\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 3.1366e-04 - mean_absolute_error: 0.0142 - val_loss: 1.2167e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.00012\n",
      "Epoch 444/450\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 2.9082e-04 - mean_absolute_error: 0.0141 - val_loss: 1.9715e-04 - val_mean_absolute_error: 0.0104\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.00012\n",
      "Epoch 445/450\n",
      "76/76 [==============================] - 25s 328ms/step - loss: 3.1582e-04 - mean_absolute_error: 0.0145 - val_loss: 1.2196e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.00012\n",
      "Epoch 446/450\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 2.9429e-04 - mean_absolute_error: 0.0141 - val_loss: 1.2953e-04 - val_mean_absolute_error: 0.0078\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.00012\n",
      "Epoch 447/450\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 3.0817e-04 - mean_absolute_error: 0.0142 - val_loss: 1.2545e-04 - val_mean_absolute_error: 0.0072\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.00012\n",
      "Epoch 448/450\n",
      "76/76 [==============================] - 24s 322ms/step - loss: 3.2475e-04 - mean_absolute_error: 0.0146 - val_loss: 1.7278e-04 - val_mean_absolute_error: 0.0100\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.00012\n",
      "Epoch 449/450\n",
      "76/76 [==============================] - 22s 294ms/step - loss: 3.1678e-04 - mean_absolute_error: 0.0145 - val_loss: 1.2392e-04 - val_mean_absolute_error: 0.0075\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.00012\n",
      "Epoch 450/450\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 3.0259e-04 - mean_absolute_error: 0.0143 - val_loss: 1.1995e-04 - val_mean_absolute_error: 0.0074\n",
      "\n",
      "Epoch 00450: val_loss improved from 0.00012 to 0.00012, saving model to results\\2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-450.h5\n"
     ]
    }
   ],
   "source": [
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "ee5b5611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "marker": {
          "color": "rgba(171, 50, 96, 0.6)"
         },
         "name": "Training Loss",
         "type": "scatter",
         "y": [
          0.0018110255477949977,
          0.0009391100611537695,
          0.0007475850870832801,
          0.0006281226524151862,
          0.0006106471410021186,
          0.0007379539893008769,
          0.0007761679589748383,
          0.0005562420701608062,
          0.0005794584867544472,
          0.000537118234205991,
          0.0005617550923489034,
          0.0005629282677546144,
          0.0005021413089707494,
          0.0005652573890984058,
          0.000569731870200485,
          0.0005445865681394935,
          0.0005293070571497083,
          0.0004831993719562888,
          0.0005029663443565369,
          0.0004769124207086861,
          0.00045125893666408956,
          0.00047136229113675654,
          0.0004676491080317646,
          0.0005253254203125834,
          0.0005326710524968803,
          0.0004730779037345201,
          0.0004509595746640116,
          0.00047383568016812205,
          0.0004465762176550925,
          0.0004149223677814007,
          0.000423033518018201,
          0.00047252626973204315,
          0.0004903415683656931,
          0.0004383819177746773,
          0.0004494936438277364,
          0.0004430588742252439,
          0.00044169017928652465,
          0.0004208444443065673,
          0.00044131785398349166,
          0.00040028608054853976,
          0.0004272366059012711,
          0.00036125225597061217,
          0.00040611662552691996,
          0.00042240318725816905,
          0.000424407422542572,
          0.00041433837031945586,
          0.0003750305622816086,
          0.0004078207421116531,
          0.00040526018710806966,
          0.00036466363235376775,
          0.0003950808313675225,
          0.00038999447133392096,
          0.00038674715324305,
          0.00035366558586247265,
          0.0003828791086561978,
          0.00039948459016159177,
          0.00040299328975379467,
          0.0003701630630530417,
          0.00038731214590370655,
          0.0003879861906170845,
          0.00038274776306934655,
          0.0003940228489227593,
          0.00037487971712835133,
          0.00038015894824638963,
          0.0003586718230508268,
          0.0003597332979552448,
          0.0003718278312589973,
          0.0003499631420709193,
          0.00038366109947673976,
          0.0003847640473395586,
          0.0003844474267680198,
          0.0003560073673725128,
          0.00035706398193724453,
          0.000407863175496459,
          0.0003527603985276073,
          0.0003918846196029335,
          0.0003770309849642217,
          0.0004103996907360852,
          0.00035926903365179896,
          0.00037084988434799016,
          0.00040143204387277365,
          0.0004168162413407117,
          0.00036092373193241656,
          0.00038485194090753794,
          0.00032346334774047136,
          0.0003730757161974907,
          0.0003290955501142889,
          0.00037538292235694826,
          0.00037511734990403056,
          0.00035297792055644095,
          0.0004133843758609146,
          0.0003626433899626136,
          0.0003783033462241292,
          0.0003626986872404814,
          0.0003554729337338358,
          0.0003764504799619317,
          0.00039068167097866535,
          0.00037331489147618413,
          0.00035133460187353194,
          0.00036746167461387813,
          0.0003397391992621124,
          0.0003629944403655827,
          0.00037101228372193873,
          0.0003487341455183923,
          0.0004209918261040002,
          0.00033896221430040896,
          0.00033485624589957297,
          0.00035303871845826507,
          0.000328150752466172,
          0.00036325101973488927,
          0.00033980689477175474,
          0.00034843539469875395,
          0.0003571454144548625,
          0.00032744102645665407,
          0.0003244895488023758,
          0.00033615430584177375,
          0.0003483146138023585,
          0.0003136794257443398,
          0.00034603982931002975,
          0.00036278649349696934,
          0.00035973245394416153,
          0.00034853717079386115,
          0.00036259021726436913,
          0.0003824296873062849,
          0.0003561618796084076,
          0.0003259293152950704,
          0.00037599491770379245,
          0.00036568520590662956,
          0.00036806316347792745,
          0.00036323157837614417,
          0.00033582383184693754,
          0.00033764788531698287,
          0.00035357335582375526,
          0.00036460961564444005,
          0.00034938749740831554,
          0.0003643553063739091,
          0.00034697557566687465,
          0.0003206293622497469,
          0.0003276545612607151,
          0.0003487952926661819,
          0.00033503485610708594,
          0.0003614295565057546,
          0.0003243743267375976,
          0.00035477138590067625,
          0.0003450801013968885,
          0.00030437912209890783,
          0.00031523575307801366,
          0.00030888651963323355,
          0.0003312684712000191,
          0.0003812589857261628,
          0.00033637642627581954,
          0.00037856478593312204,
          0.00037342534051276743,
          0.00036704298690892756,
          0.00033882693969644606,
          0.00030764195253141224,
          0.0003308317100163549,
          0.0003359345719218254,
          0.00031922096968628466,
          0.0003595257585402578,
          0.0003386377065908164,
          0.00032584546715952456,
          0.00033693353179842234,
          0.00033139498555101454,
          0.0003505750501062721,
          0.000345170235959813,
          0.0003616076719481498,
          0.00035502095124684274,
          0.00032664532773196697,
          0.00031562725780531764,
          0.00034124040394090116,
          0.00034767924807965755,
          0.0003355908556841314,
          0.00035410633427090943,
          0.0003256450581829995,
          0.0003238862846046686,
          0.00033497734693810344,
          0.00031977027538232505,
          0.0003526708751451224,
          0.00031517547904513776,
          0.000338749639922753,
          0.0003491119423415512,
          0.00032139287213794887,
          0.0003254444454796612,
          0.0003427061019465327,
          0.00034301841515116394,
          0.0003223253879696131,
          0.000367516913684085,
          0.00034098527976311743,
          0.00030283420346677303,
          0.0003080913738813251,
          0.00030915497336536646,
          0.00037610725848935544,
          0.00031568281701765954,
          0.00031528464751318097,
          0.00032622762955725193,
          0.0003241742087993771,
          0.0003342407289892435,
          0.00029929124866612256,
          0.0003488285874482244,
          0.0003075262065976858,
          0.00033616184373386204,
          0.00030791203607805073,
          0.0003374012594576925,
          0.0003540920151863247,
          0.0003230422444175929,
          0.00033242302015423775,
          0.00034111778950318694,
          0.0003374909283593297,
          0.00033179487218149006,
          0.00031838100403547287,
          0.0003474137047305703,
          0.00032854871824383736,
          0.0003024924953933805,
          0.0003324637364130467,
          0.0003322554111946374,
          0.000309775467030704,
          0.0003232752496842295,
          0.0003028715727850795,
          0.000321296916808933,
          0.000313351716613397,
          0.000290880590910092,
          0.0003246236592531204,
          0.0003287695290055126,
          0.0003120387264061719,
          0.0003256674681324512,
          0.00031124131055548787,
          0.0003310868050903082,
          0.0003308103187009692,
          0.0003011874505318701,
          0.00035576807567849755,
          0.00033236853778362274,
          0.00031712037161923945,
          0.00033189464011229575,
          0.0003505262138787657,
          0.00033758196514099836,
          0.0003214312600903213,
          0.0003282034595031291,
          0.00028930441476404667,
          0.00034673738991841674,
          0.0003174860030412674,
          0.0003105724463239312,
          0.00030203358619473875,
          0.0003387244651094079,
          0.0003120815963484347,
          0.00030312620219774544,
          0.00033167621586471796,
          0.0003103477065451443,
          0.00030457021784968674,
          0.0003050543018616736,
          0.00033132435055449605,
          0.00036536273546516895,
          0.0003351101477164775,
          0.0003176302125211805,
          0.00031284798751585186,
          0.00028838636353611946,
          0.00031723559368401766,
          0.00032528434530831873,
          0.00032488195574842393,
          0.00031625706469640136,
          0.00033767372951842844,
          0.0003344355209264904,
          0.0003383948642294854,
          0.00031427579233422875,
          0.000321108935168013,
          0.00034271180629730225,
          0.0003170412383042276,
          0.0003358708927407861,
          0.00033101742155849934,
          0.0003105413925368339,
          0.00032820767955854535,
          0.0003338334208820015,
          0.00033475662348791957,
          0.00031070280238054693,
          0.00032589706825092435,
          0.000334673241013661,
          0.0003297683724667877,
          0.0002994134265463799,
          0.0003065630153287202,
          0.00029149636975489557,
          0.00029483844991773367,
          0.00029136898228898644,
          0.00030351083842106164,
          0.0003231962618883699,
          0.00035049940925091505,
          0.0003083343617618084,
          0.00031221972312778234,
          0.00032527296571061015,
          0.00031851092353463173,
          0.0003161336062476039,
          0.0003198555205017328,
          0.00033007783349603415,
          0.0002991690707858652,
          0.00029983368585817516,
          0.0003175246820319444,
          0.0003015903930645436,
          0.0003045643388759345,
          0.0003197615151293576,
          0.00030825225985608995,
          0.0003335954970680177,
          0.0003184278612025082,
          0.0003094170242547989,
          0.0003247341082897037,
          0.00037464097840711474,
          0.00031391516677103937,
          0.00029832060681656003,
          0.00031128458795137703,
          0.0003358920512255281,
          0.00030899466946721077,
          0.0003298320807516575,
          0.00031798999407328665,
          0.00033672471181489527,
          0.000313650380121544,
          0.0003139763721264899,
          0.00032366145751439035,
          0.00032185474992729723,
          0.0003070094098802656,
          0.00029825358069501817,
          0.0003007006598636508,
          0.0003262004174757749,
          0.0002939915575552732,
          0.00031810489599592984,
          0.0003236338961869478,
          0.0002931044436991215,
          0.00032407077378593385,
          0.000300957472063601,
          0.0003486266068648547,
          0.00030183352646417916,
          0.00031854090048000216,
          0.00031620223307982087,
          0.0003172607393935323,
          0.00029892861493863165,
          0.0002905452565755695,
          0.00031937952735461295,
          0.0003146601084154099,
          0.00029649597126990557,
          0.00029392351279966533,
          0.00033826162689365447,
          0.0002872477052733302,
          0.00030688868719153106,
          0.00032323822961188853,
          0.00028414311236701906,
          0.00030102510936558247,
          0.0003100705216638744,
          0.00032427808037027717,
          0.0003205177781637758,
          0.00034077870077453554,
          0.00029272257233969867,
          0.00031418976141139865,
          0.00031103371293284,
          0.00030289645656012,
          0.00030660457559861243,
          0.00030546184279955924,
          0.0003142943314742297,
          0.0002966915490105748,
          0.00030736479675397277,
          0.00030771162710152566,
          0.00030834233621135354,
          0.0003009135543834418,
          0.00032777071464806795,
          0.0003147875831928104,
          0.0003209386777598411,
          0.00029962992994114757,
          0.00030412228079512715,
          0.0003414349048398435,
          0.00029094371711835265,
          0.0003098860033787787,
          0.00030580483144149184,
          0.00031195918563753366,
          0.0002869336458388716,
          0.00031655505881644785,
          0.0003117244050372392,
          0.0003012889646925032,
          0.0002934285730589181,
          0.00030964857432991266,
          0.0003144593210890889,
          0.000301545049296692,
          0.0003029400249943137,
          0.0003144674701616168,
          0.0002918756508734077,
          0.00034940222394652665,
          0.0003651347942650318,
          0.0003039840667042881,
          0.00029547230224125087,
          0.0002867504954338074,
          0.0002949313202407211,
          0.000311412091832608,
          0.0003099447349086404,
          0.00031160967773757875,
          0.0002967582258861512,
          0.0003128783719148487,
          0.00030476634856313467,
          0.0003358689427841455,
          0.0003268595028202981,
          0.0003195737081114203,
          0.00029320380417630076,
          0.0003215963952243328,
          0.0002770292339846492,
          0.0003113477141596377,
          0.0003311374457553029,
          0.0003028085920959711,
          0.0002951017813757062,
          0.0003331387124489993,
          0.0003053152177017182,
          0.0003068723890464753,
          0.00030387696460820735,
          0.0003135584120173007,
          0.00029766547959297895,
          0.00030382125987671316,
          0.00028754494269378483,
          0.00034512963611632586,
          0.0002990771899931133,
          0.0003037323767784983,
          0.00029097244259901345,
          0.0003250323352403939,
          0.00030769777367822826,
          0.0002954298979602754,
          0.00031922225025482476,
          0.0003133585851173848,
          0.00029464642284438014,
          0.0003396243555471301,
          0.00028764354647137225,
          0.00031619227956980467,
          0.00030325521947816014,
          0.0002868897281587124,
          0.0003220115031581372,
          0.0002845327544491738,
          0.00029416248435154557,
          0.00028997607296332717,
          0.00032478731009177864,
          0.0003204121603630483,
          0.0003081587783526629,
          0.00030104551115073264,
          0.00033286321558989584,
          0.0003131608827970922,
          0.00030612756381742656,
          0.000293455901555717,
          0.00031225490965880454,
          0.00029287696816027164,
          0.00033919946872629225,
          0.0002860514505300671,
          0.0002982637961395085,
          0.0003136560844723135,
          0.00029082392575219274,
          0.00031582009978592396,
          0.00029428544803522527,
          0.0003081665781792253,
          0.0003247520071454346,
          0.00031678154482506216,
          0.00030259034247137606
         ]
        },
        {
         "marker": {
          "color": "rgba(12, 50, 196, 0.6)"
         },
         "name": "Validation Loss",
         "type": "scatter",
         "y": [
          0.00022945010277908295,
          0.000338888872647658,
          0.0005778499762527645,
          0.00018795285723172128,
          0.00022467966482508928,
          0.0002189217775594443,
          0.00030775199411436915,
          0.0002487563469912857,
          0.00023791058629285544,
          0.00034954657894559205,
          0.00020634150132536888,
          0.00023927248548716307,
          0.00031312505598179996,
          0.00023978992248885334,
          0.00020372254948597401,
          0.0002703153295442462,
          0.0001802416954888031,
          0.00022865377832204103,
          0.0002948228211607784,
          0.0001924224925460294,
          0.000179648632183671,
          0.00030182607588358223,
          0.000191251136129722,
          0.0004749474173877388,
          0.00034462433541193604,
          0.0002085285959765315,
          0.00022158397769089788,
          0.0001909938728203997,
          0.00017733454296831042,
          0.00024124477931763977,
          0.00026963811251334846,
          0.00024169233802240342,
          0.00019321191939525306,
          0.00017452143947593868,
          0.00018150312826037407,
          0.00020132541249040514,
          0.00016753612726461142,
          0.00021713196474593133,
          0.00021861684217583388,
          0.00021621397172566503,
          0.0002428537409286946,
          0.00022457823797594756,
          0.00027099953149445355,
          0.0003408077172935009,
          0.0001841939374571666,
          0.00023814014275558293,
          0.00015209533739835024,
          0.00016135003534145653,
          0.00021475140238180757,
          0.00015099636220838875,
          0.0001701394357951358,
          0.00015430551138706505,
          0.00014656029816251248,
          0.0001649168407311663,
          0.00013707582547795027,
          0.00015637472097296268,
          0.0001607138110557571,
          0.00015042765880934894,
          0.0003140096669085324,
          0.00014406001719180495,
          0.00016455510922241956,
          0.00016480775957461447,
          0.00015626760432496667,
          0.0002377952914685011,
          0.00014560119598172605,
          0.00016595666238572448,
          0.00018911015649791807,
          0.00022313330555334687,
          0.00017142700380645692,
          0.00014208389620762318,
          0.0001306472549913451,
          0.00012951300595887005,
          0.00018435035599395633,
          0.00016233575297519565,
          0.0001337873691227287,
          0.000138188261189498,
          0.00015568756498396397,
          0.00018274343165103346,
          0.00026108778547495604,
          0.00017641548765823245,
          0.00014846590056549758,
          0.00013152252358850092,
          0.00018111185636371374,
          0.00019836729916278273,
          0.00017857873172033578,
          0.0001337896683253348,
          0.00019066066306550056,
          0.00016991766460705549,
          0.00020961558038834482,
          0.00019649683963507414,
          0.00015836588863749057,
          0.00020101868722122163,
          0.00013227581803221256,
          0.00014932130579836667,
          0.00016934853920247406,
          0.0001287429768126458,
          0.00019294951925985515,
          0.00016262799908872694,
          0.0001434927253285423,
          0.0001768965885275975,
          0.00020661362214013934,
          0.00015662344230804592,
          0.000131468492327258,
          0.0001470615970902145,
          0.00017491431208327413,
          0.00016755179967731237,
          0.0001683747541392222,
          0.0001553403417346999,
          0.0001450488780392334,
          0.00020339124603196979,
          0.00013571351883001626,
          0.00013601132377516478,
          0.00019706370949279517,
          0.00015421795251313597,
          0.00016295416571665555,
          0.0001467645342927426,
          0.0001845477963797748,
          0.0001364361378364265,
          0.00015266383707057685,
          0.00016248130123130977,
          0.00014101383567322046,
          0.000137051785713993,
          0.00014789932174608111,
          0.00013928137195762247,
          0.00016420915198978037,
          0.00017709560052026063,
          0.00016838108422234654,
          0.00019293608784209937,
          0.00018297124188393354,
          0.00014549675688613206,
          0.0001384343922836706,
          0.00016611845057923347,
          0.00014658017607871443,
          0.0001333022373728454,
          0.00019627250730991364,
          0.0001668573822826147,
          0.0001305288024013862,
          0.0001531833113403991,
          0.00015058049757499248,
          0.00012979817984160036,
          0.00020138711261097342,
          0.00013818823208566755,
          0.00014004333934281021,
          0.0002138757408829406,
          0.00015649425040464848,
          0.00016515469178557396,
          0.0001506375556346029,
          0.00013809799565933645,
          0.00013772593229077756,
          0.00013243957073427737,
          0.00013895270240027457,
          0.00023060092644300312,
          0.00016686365415807813,
          0.00013048471009824425,
          0.0001372300903312862,
          0.0002425083366688341,
          0.00013242544082459062,
          0.0001657422981224954,
          0.00014144529995974153,
          0.00012863361916970462,
          0.00015926537162158638,
          0.00013766738993581384,
          0.00015638803597539663,
          0.00012781948316842318,
          0.00019612831238191575,
          0.0001774295960785821,
          0.00014914132771082222,
          0.00013343409227672964,
          0.00013317601406015456,
          0.00016519166820216924,
          0.0001278417621506378,
          0.0001297027338296175,
          0.00014274530985858291,
          0.0001356028369627893,
          0.0001520572986919433,
          0.00013719698472414166,
          0.0001405917719239369,
          0.00013569954899139702,
          0.00017879829101730138,
          0.00013246078742668033,
          0.00022658688249066472,
          0.0001923645322676748,
          0.00015888764755800366,
          0.00014270536485128105,
          0.00013219716493040323,
          0.00018634831940289587,
          0.00020471021707635373,
          0.00014211890811566263,
          0.00012734986376017332,
          0.0001320599258178845,
          0.00012970564421266317,
          0.0001328853250015527,
          0.0001561060780659318,
          0.00013798661530017853,
          0.0002217580477008596,
          0.00013096508337184787,
          0.00013897893950343132,
          0.0001665139279793948,
          0.00013094631140120327,
          0.00015837779210414737,
          0.00013185091665945947,
          0.00013720332935918123,
          0.00016649800818413496,
          0.0001781498285708949,
          0.00013277177640702575,
          0.00013865109940525144,
          0.00012933526886627078,
          0.00012991724361199886,
          0.00013767062046099454,
          0.0001422567875124514,
          0.00012903417518828064,
          0.00014211307279765606,
          0.0001261206780327484,
          0.00016110714932437986,
          0.00014393504534382373,
          0.00015827424067538232,
          0.00014798197662457824,
          0.00015613292634952813,
          0.00020155461970716715,
          0.00013786544150207192,
          0.0001450612035114318,
          0.00013945068349130452,
          0.0001417903258698061,
          0.00013262679567560554,
          0.0001491019211243838,
          0.00013102416414767504,
          0.00021036085672676563,
          0.00015105736383702606,
          0.0001402840280206874,
          0.0001438241743016988,
          0.0001646107411943376,
          0.00015281804371625185,
          0.00016076778410933912,
          0.0001824688515625894,
          0.00013268458133097738,
          0.0001290876098209992,
          0.00017333278083242476,
          0.0001313542597927153,
          0.0001378573797410354,
          0.0001266795297851786,
          0.0002350359718548134,
          0.00019509390403982252,
          0.00016280182171612978,
          0.00013510137796401978,
          0.0001284058962482959,
          0.00012996778241358697,
          0.00015600457845721394,
          0.00016910403792280704,
          0.0001459690829506144,
          0.00013316157856024802,
          0.0001324268087046221,
          0.00012718132347799838,
          0.0001749529328662902,
          0.00016039269394241273,
          0.00012420615530572832,
          0.0001268890919163823,
          0.0001353134139208123,
          0.00014205767365638167,
          0.0001837339805206284,
          0.00017763515643309802,
          0.0001728146307868883,
          0.0001866927632363513,
          0.0002658147714100778,
          0.0001837674353737384,
          0.00013157255307305604,
          0.00016785676416475326,
          0.0001445996604161337,
          0.00016176185454241931,
          0.00013766933989245445,
          0.00016678428801242262,
          0.00014236937568057328,
          0.00014094159996602684,
          0.00013023354404140264,
          0.00014597439439967275,
          0.0001475450408179313,
          0.00012461162987165153,
          0.0001311885571340099,
          0.00016407444491051137,
          0.00012887349294032902,
          0.00013985915575176477,
          0.00014099902182351798,
          0.0001328436628682539,
          0.00017616366676520556,
          0.00013637480151373893,
          0.00012870562204625458,
          0.0001521967351436615,
          0.00012728881847579032,
          0.00012913534010294825,
          0.00013320549624040723,
          0.00013544599642045796,
          0.00016795279225334525,
          0.0001331391540588811,
          0.00013640943507198244,
          0.00013054754526820034,
          0.00014593165542464703,
          0.00012909377983305603,
          0.00013705294986721128,
          0.00012565446377266198,
          0.00013823961489833891,
          0.00017989147454500198,
          0.00015998471644707024,
          0.0001445644738851115,
          0.0002631804090924561,
          0.00014694631681777537,
          0.00014303790521807969,
          0.00014244428894016892,
          0.000145471902214922,
          0.00015199751942418516,
          0.00015037627599667758,
          0.00019076958415098488,
          0.00013787570060230792,
          0.0001472395524615422,
          0.0001334651606157422,
          0.00012476128176786005,
          0.00013910185953136533,
          0.00014079856919124722,
          0.00013679957191925496,
          0.00013594610209111124,
          0.00013294060772750527,
          0.00012911409430671483,
          0.00013265527377370745,
          0.0001474381861044094,
          0.0001308871724177152,
          0.00013470039993990213,
          0.0001494925090810284,
          0.00014290810213424265,
          0.00014269558596424758,
          0.00013700647104997188,
          0.00014773276052437723,
          0.00012445062748156488,
          0.00012947349750902504,
          0.00014174562238622457,
          0.00014088195166550577,
          0.00014589242346119136,
          0.0001312281674472615,
          0.00012569609680213034,
          0.00013279255654197186,
          0.0001293542591156438,
          0.00013276291429065168,
          0.00013971104635857046,
          0.00015960690507199615,
          0.00013453522115014493,
          0.0001427571551175788,
          0.00012299175432417542,
          0.00016247307939920574,
          0.00012709952716249973,
          0.00013589239097200334,
          0.0001436837628716603,
          0.00013050425332039595,
          0.00020269730885047466,
          0.00012595357839018106,
          0.00013180360838305205,
          0.0001375280844513327,
          0.00012606939708348364,
          0.00013030593981966376,
          0.000157183560077101,
          0.00012877906556241214,
          0.00017750522238202393,
          0.00013342723832465708,
          0.0001432706048944965,
          0.00014730615657754242,
          0.00014488294254988432,
          0.00014866738638374954,
          0.0001703063608147204,
          0.00014663209731224924,
          0.00012761913239955902,
          0.00012916314881294966,
          0.00012856103421654552,
          0.00014328541874419898,
          0.0001325166958849877,
          0.00018323569383937865,
          0.00018992579134646803,
          0.00015772049664519727,
          0.0001536919007776305,
          0.00013600716192740947,
          0.00015880074352025986,
          0.00014203620958141983,
          0.00012856809189543128,
          0.00013489821867551655,
          0.00013313328963704407,
          0.0001519254146842286,
          0.00017365273379255086,
          0.00013602538092527539,
          0.00014078334788791835,
          0.0001304784818785265,
          0.00012655557657126337,
          0.00014495821960736066,
          0.0002210759703302756,
          0.00013275320816319436,
          0.00012624726514331996,
          0.00012329814489930868,
          0.00012468920613173395,
          0.00016019675240386277,
          0.00014701977488584816,
          0.00012700329534709454,
          0.0001462197833461687,
          0.00013660262629855424,
          0.00012799490650650114,
          0.00012233902816660702,
          0.00013135053450241685,
          0.00012349433382041752,
          0.00018287784769199789,
          0.00013589090667665005,
          0.00014874161570332944,
          0.0001254141825484112,
          0.00013008690439164639,
          0.00014157277473714203,
          0.00015494653780478984,
          0.0001640373666305095,
          0.00015600683400407434,
          0.00013081695942673832,
          0.0001401264307787642,
          0.00013752713857684284,
          0.00012177894677734002,
          0.000133714362164028,
          0.00014141936844680458,
          0.00015249881835188717,
          0.00012737569340970367,
          0.00013728866179008037,
          0.00013176524953451008,
          0.0001264214370166883,
          0.00012637808686122298,
          0.0001334679836872965,
          0.00013472605496644974,
          0.000127980369143188,
          0.00012554861314129084,
          0.00012080533633707091,
          0.00012931674427818507,
          0.00013105459220241755,
          0.00016486228560097516,
          0.00014546760939992964,
          0.0001324049080722034,
          0.00012201406934764236,
          0.00012573186540976167,
          0.00014600451686419547,
          0.00012569745013024658,
          0.00013160856906324625,
          0.00012625822273548692,
          0.00016636276268400252,
          0.00014754667063243687,
          0.00012806340237148106,
          0.00013124221004545689,
          0.00012166779924882576,
          0.0001971487799892202,
          0.00012195932504255325,
          0.00012953071563970298,
          0.00012545443314593285,
          0.00017277678125537932,
          0.00012392491044010967,
          0.00011995492241112515
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Loss and Val_Loss in 500 Epochs"
        },
        "xaxis": {
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"893c6e9a-3f83-492f-a003-eaab4c188983\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';                                    if (document.getElementById(\"893c6e9a-3f83-492f-a003-eaab4c188983\")) {                    Plotly.newPlot(                        \"893c6e9a-3f83-492f-a003-eaab4c188983\",                        [{\"marker\":{\"color\":\"rgba(171, 50, 96, 0.6)\"},\"name\":\"Training Loss\",\"type\":\"scatter\",\"y\":[0.0018110255477949977,0.0009391100611537695,0.0007475850870832801,0.0006281226524151862,0.0006106471410021186,0.0007379539893008769,0.0007761679589748383,0.0005562420701608062,0.0005794584867544472,0.000537118234205991,0.0005617550923489034,0.0005629282677546144,0.0005021413089707494,0.0005652573890984058,0.000569731870200485,0.0005445865681394935,0.0005293070571497083,0.0004831993719562888,0.0005029663443565369,0.0004769124207086861,0.00045125893666408956,0.00047136229113675654,0.0004676491080317646,0.0005253254203125834,0.0005326710524968803,0.0004730779037345201,0.0004509595746640116,0.00047383568016812205,0.0004465762176550925,0.0004149223677814007,0.000423033518018201,0.00047252626973204315,0.0004903415683656931,0.0004383819177746773,0.0004494936438277364,0.0004430588742252439,0.00044169017928652465,0.0004208444443065673,0.00044131785398349166,0.00040028608054853976,0.0004272366059012711,0.00036125225597061217,0.00040611662552691996,0.00042240318725816905,0.000424407422542572,0.00041433837031945586,0.0003750305622816086,0.0004078207421116531,0.00040526018710806966,0.00036466363235376775,0.0003950808313675225,0.00038999447133392096,0.00038674715324305,0.00035366558586247265,0.0003828791086561978,0.00039948459016159177,0.00040299328975379467,0.0003701630630530417,0.00038731214590370655,0.0003879861906170845,0.00038274776306934655,0.0003940228489227593,0.00037487971712835133,0.00038015894824638963,0.0003586718230508268,0.0003597332979552448,0.0003718278312589973,0.0003499631420709193,0.00038366109947673976,0.0003847640473395586,0.0003844474267680198,0.0003560073673725128,0.00035706398193724453,0.000407863175496459,0.0003527603985276073,0.0003918846196029335,0.0003770309849642217,0.0004103996907360852,0.00035926903365179896,0.00037084988434799016,0.00040143204387277365,0.0004168162413407117,0.00036092373193241656,0.00038485194090753794,0.00032346334774047136,0.0003730757161974907,0.0003290955501142889,0.00037538292235694826,0.00037511734990403056,0.00035297792055644095,0.0004133843758609146,0.0003626433899626136,0.0003783033462241292,0.0003626986872404814,0.0003554729337338358,0.0003764504799619317,0.00039068167097866535,0.00037331489147618413,0.00035133460187353194,0.00036746167461387813,0.0003397391992621124,0.0003629944403655827,0.00037101228372193873,0.0003487341455183923,0.0004209918261040002,0.00033896221430040896,0.00033485624589957297,0.00035303871845826507,0.000328150752466172,0.00036325101973488927,0.00033980689477175474,0.00034843539469875395,0.0003571454144548625,0.00032744102645665407,0.0003244895488023758,0.00033615430584177375,0.0003483146138023585,0.0003136794257443398,0.00034603982931002975,0.00036278649349696934,0.00035973245394416153,0.00034853717079386115,0.00036259021726436913,0.0003824296873062849,0.0003561618796084076,0.0003259293152950704,0.00037599491770379245,0.00036568520590662956,0.00036806316347792745,0.00036323157837614417,0.00033582383184693754,0.00033764788531698287,0.00035357335582375526,0.00036460961564444005,0.00034938749740831554,0.0003643553063739091,0.00034697557566687465,0.0003206293622497469,0.0003276545612607151,0.0003487952926661819,0.00033503485610708594,0.0003614295565057546,0.0003243743267375976,0.00035477138590067625,0.0003450801013968885,0.00030437912209890783,0.00031523575307801366,0.00030888651963323355,0.0003312684712000191,0.0003812589857261628,0.00033637642627581954,0.00037856478593312204,0.00037342534051276743,0.00036704298690892756,0.00033882693969644606,0.00030764195253141224,0.0003308317100163549,0.0003359345719218254,0.00031922096968628466,0.0003595257585402578,0.0003386377065908164,0.00032584546715952456,0.00033693353179842234,0.00033139498555101454,0.0003505750501062721,0.000345170235959813,0.0003616076719481498,0.00035502095124684274,0.00032664532773196697,0.00031562725780531764,0.00034124040394090116,0.00034767924807965755,0.0003355908556841314,0.00035410633427090943,0.0003256450581829995,0.0003238862846046686,0.00033497734693810344,0.00031977027538232505,0.0003526708751451224,0.00031517547904513776,0.000338749639922753,0.0003491119423415512,0.00032139287213794887,0.0003254444454796612,0.0003427061019465327,0.00034301841515116394,0.0003223253879696131,0.000367516913684085,0.00034098527976311743,0.00030283420346677303,0.0003080913738813251,0.00030915497336536646,0.00037610725848935544,0.00031568281701765954,0.00031528464751318097,0.00032622762955725193,0.0003241742087993771,0.0003342407289892435,0.00029929124866612256,0.0003488285874482244,0.0003075262065976858,0.00033616184373386204,0.00030791203607805073,0.0003374012594576925,0.0003540920151863247,0.0003230422444175929,0.00033242302015423775,0.00034111778950318694,0.0003374909283593297,0.00033179487218149006,0.00031838100403547287,0.0003474137047305703,0.00032854871824383736,0.0003024924953933805,0.0003324637364130467,0.0003322554111946374,0.000309775467030704,0.0003232752496842295,0.0003028715727850795,0.000321296916808933,0.000313351716613397,0.000290880590910092,0.0003246236592531204,0.0003287695290055126,0.0003120387264061719,0.0003256674681324512,0.00031124131055548787,0.0003310868050903082,0.0003308103187009692,0.0003011874505318701,0.00035576807567849755,0.00033236853778362274,0.00031712037161923945,0.00033189464011229575,0.0003505262138787657,0.00033758196514099836,0.0003214312600903213,0.0003282034595031291,0.00028930441476404667,0.00034673738991841674,0.0003174860030412674,0.0003105724463239312,0.00030203358619473875,0.0003387244651094079,0.0003120815963484347,0.00030312620219774544,0.00033167621586471796,0.0003103477065451443,0.00030457021784968674,0.0003050543018616736,0.00033132435055449605,0.00036536273546516895,0.0003351101477164775,0.0003176302125211805,0.00031284798751585186,0.00028838636353611946,0.00031723559368401766,0.00032528434530831873,0.00032488195574842393,0.00031625706469640136,0.00033767372951842844,0.0003344355209264904,0.0003383948642294854,0.00031427579233422875,0.000321108935168013,0.00034271180629730225,0.0003170412383042276,0.0003358708927407861,0.00033101742155849934,0.0003105413925368339,0.00032820767955854535,0.0003338334208820015,0.00033475662348791957,0.00031070280238054693,0.00032589706825092435,0.000334673241013661,0.0003297683724667877,0.0002994134265463799,0.0003065630153287202,0.00029149636975489557,0.00029483844991773367,0.00029136898228898644,0.00030351083842106164,0.0003231962618883699,0.00035049940925091505,0.0003083343617618084,0.00031221972312778234,0.00032527296571061015,0.00031851092353463173,0.0003161336062476039,0.0003198555205017328,0.00033007783349603415,0.0002991690707858652,0.00029983368585817516,0.0003175246820319444,0.0003015903930645436,0.0003045643388759345,0.0003197615151293576,0.00030825225985608995,0.0003335954970680177,0.0003184278612025082,0.0003094170242547989,0.0003247341082897037,0.00037464097840711474,0.00031391516677103937,0.00029832060681656003,0.00031128458795137703,0.0003358920512255281,0.00030899466946721077,0.0003298320807516575,0.00031798999407328665,0.00033672471181489527,0.000313650380121544,0.0003139763721264899,0.00032366145751439035,0.00032185474992729723,0.0003070094098802656,0.00029825358069501817,0.0003007006598636508,0.0003262004174757749,0.0002939915575552732,0.00031810489599592984,0.0003236338961869478,0.0002931044436991215,0.00032407077378593385,0.000300957472063601,0.0003486266068648547,0.00030183352646417916,0.00031854090048000216,0.00031620223307982087,0.0003172607393935323,0.00029892861493863165,0.0002905452565755695,0.00031937952735461295,0.0003146601084154099,0.00029649597126990557,0.00029392351279966533,0.00033826162689365447,0.0002872477052733302,0.00030688868719153106,0.00032323822961188853,0.00028414311236701906,0.00030102510936558247,0.0003100705216638744,0.00032427808037027717,0.0003205177781637758,0.00034077870077453554,0.00029272257233969867,0.00031418976141139865,0.00031103371293284,0.00030289645656012,0.00030660457559861243,0.00030546184279955924,0.0003142943314742297,0.0002966915490105748,0.00030736479675397277,0.00030771162710152566,0.00030834233621135354,0.0003009135543834418,0.00032777071464806795,0.0003147875831928104,0.0003209386777598411,0.00029962992994114757,0.00030412228079512715,0.0003414349048398435,0.00029094371711835265,0.0003098860033787787,0.00030580483144149184,0.00031195918563753366,0.0002869336458388716,0.00031655505881644785,0.0003117244050372392,0.0003012889646925032,0.0002934285730589181,0.00030964857432991266,0.0003144593210890889,0.000301545049296692,0.0003029400249943137,0.0003144674701616168,0.0002918756508734077,0.00034940222394652665,0.0003651347942650318,0.0003039840667042881,0.00029547230224125087,0.0002867504954338074,0.0002949313202407211,0.000311412091832608,0.0003099447349086404,0.00031160967773757875,0.0002967582258861512,0.0003128783719148487,0.00030476634856313467,0.0003358689427841455,0.0003268595028202981,0.0003195737081114203,0.00029320380417630076,0.0003215963952243328,0.0002770292339846492,0.0003113477141596377,0.0003311374457553029,0.0003028085920959711,0.0002951017813757062,0.0003331387124489993,0.0003053152177017182,0.0003068723890464753,0.00030387696460820735,0.0003135584120173007,0.00029766547959297895,0.00030382125987671316,0.00028754494269378483,0.00034512963611632586,0.0002990771899931133,0.0003037323767784983,0.00029097244259901345,0.0003250323352403939,0.00030769777367822826,0.0002954298979602754,0.00031922225025482476,0.0003133585851173848,0.00029464642284438014,0.0003396243555471301,0.00028764354647137225,0.00031619227956980467,0.00030325521947816014,0.0002868897281587124,0.0003220115031581372,0.0002845327544491738,0.00029416248435154557,0.00028997607296332717,0.00032478731009177864,0.0003204121603630483,0.0003081587783526629,0.00030104551115073264,0.00033286321558989584,0.0003131608827970922,0.00030612756381742656,0.000293455901555717,0.00031225490965880454,0.00029287696816027164,0.00033919946872629225,0.0002860514505300671,0.0002982637961395085,0.0003136560844723135,0.00029082392575219274,0.00031582009978592396,0.00029428544803522527,0.0003081665781792253,0.0003247520071454346,0.00031678154482506216,0.00030259034247137606]},{\"marker\":{\"color\":\"rgba(12, 50, 196, 0.6)\"},\"name\":\"Validation Loss\",\"type\":\"scatter\",\"y\":[0.00022945010277908295,0.000338888872647658,0.0005778499762527645,0.00018795285723172128,0.00022467966482508928,0.0002189217775594443,0.00030775199411436915,0.0002487563469912857,0.00023791058629285544,0.00034954657894559205,0.00020634150132536888,0.00023927248548716307,0.00031312505598179996,0.00023978992248885334,0.00020372254948597401,0.0002703153295442462,0.0001802416954888031,0.00022865377832204103,0.0002948228211607784,0.0001924224925460294,0.000179648632183671,0.00030182607588358223,0.000191251136129722,0.0004749474173877388,0.00034462433541193604,0.0002085285959765315,0.00022158397769089788,0.0001909938728203997,0.00017733454296831042,0.00024124477931763977,0.00026963811251334846,0.00024169233802240342,0.00019321191939525306,0.00017452143947593868,0.00018150312826037407,0.00020132541249040514,0.00016753612726461142,0.00021713196474593133,0.00021861684217583388,0.00021621397172566503,0.0002428537409286946,0.00022457823797594756,0.00027099953149445355,0.0003408077172935009,0.0001841939374571666,0.00023814014275558293,0.00015209533739835024,0.00016135003534145653,0.00021475140238180757,0.00015099636220838875,0.0001701394357951358,0.00015430551138706505,0.00014656029816251248,0.0001649168407311663,0.00013707582547795027,0.00015637472097296268,0.0001607138110557571,0.00015042765880934894,0.0003140096669085324,0.00014406001719180495,0.00016455510922241956,0.00016480775957461447,0.00015626760432496667,0.0002377952914685011,0.00014560119598172605,0.00016595666238572448,0.00018911015649791807,0.00022313330555334687,0.00017142700380645692,0.00014208389620762318,0.0001306472549913451,0.00012951300595887005,0.00018435035599395633,0.00016233575297519565,0.0001337873691227287,0.000138188261189498,0.00015568756498396397,0.00018274343165103346,0.00026108778547495604,0.00017641548765823245,0.00014846590056549758,0.00013152252358850092,0.00018111185636371374,0.00019836729916278273,0.00017857873172033578,0.0001337896683253348,0.00019066066306550056,0.00016991766460705549,0.00020961558038834482,0.00019649683963507414,0.00015836588863749057,0.00020101868722122163,0.00013227581803221256,0.00014932130579836667,0.00016934853920247406,0.0001287429768126458,0.00019294951925985515,0.00016262799908872694,0.0001434927253285423,0.0001768965885275975,0.00020661362214013934,0.00015662344230804592,0.000131468492327258,0.0001470615970902145,0.00017491431208327413,0.00016755179967731237,0.0001683747541392222,0.0001553403417346999,0.0001450488780392334,0.00020339124603196979,0.00013571351883001626,0.00013601132377516478,0.00019706370949279517,0.00015421795251313597,0.00016295416571665555,0.0001467645342927426,0.0001845477963797748,0.0001364361378364265,0.00015266383707057685,0.00016248130123130977,0.00014101383567322046,0.000137051785713993,0.00014789932174608111,0.00013928137195762247,0.00016420915198978037,0.00017709560052026063,0.00016838108422234654,0.00019293608784209937,0.00018297124188393354,0.00014549675688613206,0.0001384343922836706,0.00016611845057923347,0.00014658017607871443,0.0001333022373728454,0.00019627250730991364,0.0001668573822826147,0.0001305288024013862,0.0001531833113403991,0.00015058049757499248,0.00012979817984160036,0.00020138711261097342,0.00013818823208566755,0.00014004333934281021,0.0002138757408829406,0.00015649425040464848,0.00016515469178557396,0.0001506375556346029,0.00013809799565933645,0.00013772593229077756,0.00013243957073427737,0.00013895270240027457,0.00023060092644300312,0.00016686365415807813,0.00013048471009824425,0.0001372300903312862,0.0002425083366688341,0.00013242544082459062,0.0001657422981224954,0.00014144529995974153,0.00012863361916970462,0.00015926537162158638,0.00013766738993581384,0.00015638803597539663,0.00012781948316842318,0.00019612831238191575,0.0001774295960785821,0.00014914132771082222,0.00013343409227672964,0.00013317601406015456,0.00016519166820216924,0.0001278417621506378,0.0001297027338296175,0.00014274530985858291,0.0001356028369627893,0.0001520572986919433,0.00013719698472414166,0.0001405917719239369,0.00013569954899139702,0.00017879829101730138,0.00013246078742668033,0.00022658688249066472,0.0001923645322676748,0.00015888764755800366,0.00014270536485128105,0.00013219716493040323,0.00018634831940289587,0.00020471021707635373,0.00014211890811566263,0.00012734986376017332,0.0001320599258178845,0.00012970564421266317,0.0001328853250015527,0.0001561060780659318,0.00013798661530017853,0.0002217580477008596,0.00013096508337184787,0.00013897893950343132,0.0001665139279793948,0.00013094631140120327,0.00015837779210414737,0.00013185091665945947,0.00013720332935918123,0.00016649800818413496,0.0001781498285708949,0.00013277177640702575,0.00013865109940525144,0.00012933526886627078,0.00012991724361199886,0.00013767062046099454,0.0001422567875124514,0.00012903417518828064,0.00014211307279765606,0.0001261206780327484,0.00016110714932437986,0.00014393504534382373,0.00015827424067538232,0.00014798197662457824,0.00015613292634952813,0.00020155461970716715,0.00013786544150207192,0.0001450612035114318,0.00013945068349130452,0.0001417903258698061,0.00013262679567560554,0.0001491019211243838,0.00013102416414767504,0.00021036085672676563,0.00015105736383702606,0.0001402840280206874,0.0001438241743016988,0.0001646107411943376,0.00015281804371625185,0.00016076778410933912,0.0001824688515625894,0.00013268458133097738,0.0001290876098209992,0.00017333278083242476,0.0001313542597927153,0.0001378573797410354,0.0001266795297851786,0.0002350359718548134,0.00019509390403982252,0.00016280182171612978,0.00013510137796401978,0.0001284058962482959,0.00012996778241358697,0.00015600457845721394,0.00016910403792280704,0.0001459690829506144,0.00013316157856024802,0.0001324268087046221,0.00012718132347799838,0.0001749529328662902,0.00016039269394241273,0.00012420615530572832,0.0001268890919163823,0.0001353134139208123,0.00014205767365638167,0.0001837339805206284,0.00017763515643309802,0.0001728146307868883,0.0001866927632363513,0.0002658147714100778,0.0001837674353737384,0.00013157255307305604,0.00016785676416475326,0.0001445996604161337,0.00016176185454241931,0.00013766933989245445,0.00016678428801242262,0.00014236937568057328,0.00014094159996602684,0.00013023354404140264,0.00014597439439967275,0.0001475450408179313,0.00012461162987165153,0.0001311885571340099,0.00016407444491051137,0.00012887349294032902,0.00013985915575176477,0.00014099902182351798,0.0001328436628682539,0.00017616366676520556,0.00013637480151373893,0.00012870562204625458,0.0001521967351436615,0.00012728881847579032,0.00012913534010294825,0.00013320549624040723,0.00013544599642045796,0.00016795279225334525,0.0001331391540588811,0.00013640943507198244,0.00013054754526820034,0.00014593165542464703,0.00012909377983305603,0.00013705294986721128,0.00012565446377266198,0.00013823961489833891,0.00017989147454500198,0.00015998471644707024,0.0001445644738851115,0.0002631804090924561,0.00014694631681777537,0.00014303790521807969,0.00014244428894016892,0.000145471902214922,0.00015199751942418516,0.00015037627599667758,0.00019076958415098488,0.00013787570060230792,0.0001472395524615422,0.0001334651606157422,0.00012476128176786005,0.00013910185953136533,0.00014079856919124722,0.00013679957191925496,0.00013594610209111124,0.00013294060772750527,0.00012911409430671483,0.00013265527377370745,0.0001474381861044094,0.0001308871724177152,0.00013470039993990213,0.0001494925090810284,0.00014290810213424265,0.00014269558596424758,0.00013700647104997188,0.00014773276052437723,0.00012445062748156488,0.00012947349750902504,0.00014174562238622457,0.00014088195166550577,0.00014589242346119136,0.0001312281674472615,0.00012569609680213034,0.00013279255654197186,0.0001293542591156438,0.00013276291429065168,0.00013971104635857046,0.00015960690507199615,0.00013453522115014493,0.0001427571551175788,0.00012299175432417542,0.00016247307939920574,0.00012709952716249973,0.00013589239097200334,0.0001436837628716603,0.00013050425332039595,0.00020269730885047466,0.00012595357839018106,0.00013180360838305205,0.0001375280844513327,0.00012606939708348364,0.00013030593981966376,0.000157183560077101,0.00012877906556241214,0.00017750522238202393,0.00013342723832465708,0.0001432706048944965,0.00014730615657754242,0.00014488294254988432,0.00014866738638374954,0.0001703063608147204,0.00014663209731224924,0.00012761913239955902,0.00012916314881294966,0.00012856103421654552,0.00014328541874419898,0.0001325166958849877,0.00018323569383937865,0.00018992579134646803,0.00015772049664519727,0.0001536919007776305,0.00013600716192740947,0.00015880074352025986,0.00014203620958141983,0.00012856809189543128,0.00013489821867551655,0.00013313328963704407,0.0001519254146842286,0.00017365273379255086,0.00013602538092527539,0.00014078334788791835,0.0001304784818785265,0.00012655557657126337,0.00014495821960736066,0.0002210759703302756,0.00013275320816319436,0.00012624726514331996,0.00012329814489930868,0.00012468920613173395,0.00016019675240386277,0.00014701977488584816,0.00012700329534709454,0.0001462197833461687,0.00013660262629855424,0.00012799490650650114,0.00012233902816660702,0.00013135053450241685,0.00012349433382041752,0.00018287784769199789,0.00013589090667665005,0.00014874161570332944,0.0001254141825484112,0.00013008690439164639,0.00014157277473714203,0.00015494653780478984,0.0001640373666305095,0.00015600683400407434,0.00013081695942673832,0.0001401264307787642,0.00013752713857684284,0.00012177894677734002,0.000133714362164028,0.00014141936844680458,0.00015249881835188717,0.00012737569340970367,0.00013728866179008037,0.00013176524953451008,0.0001264214370166883,0.00012637808686122298,0.0001334679836872965,0.00013472605496644974,0.000127980369143188,0.00012554861314129084,0.00012080533633707091,0.00012931674427818507,0.00013105459220241755,0.00016486228560097516,0.00014546760939992964,0.0001324049080722034,0.00012201406934764236,0.00012573186540976167,0.00014600451686419547,0.00012569745013024658,0.00013160856906324625,0.00012625822273548692,0.00016636276268400252,0.00014754667063243687,0.00012806340237148106,0.00013124221004545689,0.00012166779924882576,0.0001971487799892202,0.00012195932504255325,0.00012953071563970298,0.00012545443314593285,0.00017277678125537932,0.00012392491044010967,0.00011995492241112515]}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Loss and Val_Loss in 500 Epochs\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('893c6e9a-3f83-492f-a003-eaab4c188983');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    y=history.history['loss'],\n",
    "    name = \"Training Loss\",\n",
    "    marker=dict(color='rgba(171, 50, 96, 0.6)'))\n",
    "trace2 = go.Scatter(\n",
    "    y=history.history['val_loss'],\n",
    "    name = \"Validation Loss\",\n",
    "    marker=dict(color='rgba(12, 50, 196, 0.6)'))\n",
    "trace3 = go.Scatter(\n",
    "    y=history.history['mean_absolute_error'],\n",
    "    name = \"MAE\",\n",
    "    marker=dict(color='rgba(212, 50, 296, 0.6)'))\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(title='Loss and Val_Loss in 500 Epochs',\n",
    "                   xaxis=dict(title='Epoch'),\n",
    "                   yaxis=dict(title='Loss'),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig, config={'showLink': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "fa282523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FROM NEWEST RUN\n",
    "\"\"\"\n",
    "\n",
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "88606520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FROM PREVIOUS RUNS\n",
    "\"\"\"\n",
    "\n",
    "name = '2021-09-06_AMZN-shuffle-1-scale-1-split_date-0-epochs-500'\n",
    "\n",
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "1852f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "83b22950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "163ec598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "5be79c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "c3e2c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "88d34b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 3333.75$\n",
      "huber_loss loss: 9.322586993221194e-05\n",
      "Mean Absolute Error: 26.435247780594498\n",
      "Accuracy score: 0.597027250206441\n",
      "Total buy profit: 11979.31767463684\n",
      "Total sell profit: 3121.9663591384888\n",
      "Total profit: 15101.28403377533\n",
      "Profit per trade: 12.470094164967241\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da19da6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
